{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34c05814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch:  True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "from abc import ABCMeta\n",
    "import argparse\n",
    "import datetime \n",
    "import collections\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import pickle\n",
    "import sys\n",
    "import io\n",
    "from tqdm import tqdm, trange\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, WeightedRandomSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
    "from pytorch_pretrained_bert.modeling import BertForQuestionAnswering\n",
    "from pytorch_pretrained_bert.modeling import BertForQuestionAnsweringQC4QA\n",
    "\n",
    "from pytorch_pretrained_bert.optimization import BertAdam\n",
    "from utils.ConfigLogger import config_logger\n",
    "from utils.evaluate import f1_score, exact_match_score, metric_max_over_ground_truths\n",
    "from utils.BERTRandomSampler import BERTRandomSampler\n",
    "\n",
    "PYTORCH_PRETRAINED_BERT_CACHE = Path(os.getenv('PYTORCH_PRETRAINED_BERT_CACHE',\n",
    "                                               Path.home() / '.pytorch_pretrained_bert'))\n",
    "\n",
    "from da_data_utils import * \n",
    "############################\n",
    "import importlib, types, argparse\n",
    "from utils.ConfigLogger import config_logger\n",
    "print('torch: ',torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b491535",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_stage(args, device, tokenizer, logger, debug=False):\n",
    "    # Load a trained model that you have fine-tuned\n",
    "    output_model_file = os.path.join(args.output_dir, args.output_model_file)\n",
    "    model_state_dict = torch.load(output_model_file)\n",
    "    model = BertForQuestionAnswering.from_pretrained(args.bert_model, state_dict=model_state_dict, args=args)\n",
    "    model.to(device)\n",
    "    # Read prediction samples\n",
    "    read_limit = None\n",
    "    if debug:\n",
    "        read_limit = 100 # 샘플 100개만 사용\n",
    "    logger.info(\"***** Reading Prediction Samples *****\")\n",
    "    eval_features, eval_examples = read_features_and_examples(args, args.predict_file, tokenizer, logger,\n",
    "            use_simple_feature=False, read_examples=True, limit=read_limit)\n",
    "    acc, f1 = evaluation_stage(args, eval_examples, eval_features, device, model, logger)\n",
    "    logger.info('***** Prediction Performance *****')\n",
    "    logger.info('EM is %.5f, F1 is %.5f', acc, f1)\n",
    "\n",
    "\n",
    "def evaluate_acc_and_f1(predictions, raw_data, logger, threshold=-1, all_probs=None):\n",
    "    f1 = exact_match = total = 0\n",
    "    eval_threshold = True\n",
    "    if threshold is None or all_probs is None:\n",
    "        eval_threshold = False\n",
    "    for sample in raw_data:\n",
    "        if (sample.qas_id not in predictions) or (eval_threshold and sample.qas_id not in all_probs):\n",
    "            message = 'Unanswered question ' + sample.qas_id + ' will receive score 0.'\n",
    "            logger.warn(message)\n",
    "            continue\n",
    "        if not eval_threshold or (eval_threshold and all_probs[sample.qas_id] >= threshold):\n",
    "            ground_truths = sample.orig_answers\n",
    "            prediction = predictions[sample.qas_id]\n",
    "            exact_match += metric_max_over_ground_truths(\n",
    "                exact_match_score, prediction, ground_truths)\n",
    "            f1 += metric_max_over_ground_truths(\n",
    "                f1_score, prediction, ground_truths)\n",
    "            total += 1\n",
    "\n",
    "    exact_match = 100.0 * exact_match / total\n",
    "    f1 = 100.0 * f1 / total\n",
    "\n",
    "    return exact_match, f1\n",
    "\n",
    "\n",
    "def keep_high_prob_samples(all_probs, all_features, prob_threshold, removed_feature_index, all_indices,\n",
    "        keep_generated=False):\n",
    "    '''\n",
    "    셀프 트레이닝용: 높은 확률의 예측을 pseudo-label로 변환\n",
    "    '''\n",
    "    new_train_features = []\n",
    "    for i, feature in enumerate(all_features):\n",
    "        if keep_generated:\n",
    "            if feature.example_index not in removed_feature_index and all_probs[feature.example_index] > prob_threshold:\n",
    "                feature.start_position, feature.end_position = all_indices[i][0] = all_indices[i][1]\n",
    "                new_train_features.append(feature)\n",
    "                removed_feature_index.add(feature.example_index)\n",
    "        else:\n",
    "            if all_probs[feature.example_index] > prob_threshold:\n",
    "                feature.start_position, feature.end_position = all_indices[i][0], all_indices[i][1]\n",
    "                new_train_features.append(feature)\n",
    "    return new_train_features, removed_feature_index\n",
    "\n",
    "\n",
    "def compare_performance(args, best_acc, best_f1, acc, f1, model, logger):\n",
    "    if not (best_f1 is None or best_acc is None):\n",
    "        if best_acc < acc:\n",
    "            logger.info('Current model BEATS previous best model, previous best is EM = %.5F, F1 = %.5f',\n",
    "                best_acc, best_f1)\n",
    "            best_acc, best_f1 = acc, f1\n",
    "            logger.info('Current best model has been saved!')\n",
    "            model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n",
    "            torch.save(model_to_save.state_dict(), os.path.join(args.output_dir, args.output_model_file))\n",
    "        else:\n",
    "            logger.info('Current model CANNOT beat previous best model, previous best is EM = %.5F, F1 = %.5f',\n",
    "                best_acc, best_f1)\n",
    "    else:\n",
    "        best_acc, best_f1 = acc, f1\n",
    "    return best_acc, best_f1\n",
    "\n",
    "\n",
    "def evaluation_stage(args, eval_examples, eval_features, device, model, logger, generate_prob_th=0.6,\n",
    "        removed_feature_index=None, global_step=None, best_acc=None, best_f1=None, generate_label=False):\n",
    "    if not global_step:\n",
    "        logger.info(\"***** Running Evaluation Stage *****\")\n",
    "    else:\n",
    "        logger.info(\"***** Running Predictions *****\")\n",
    "    logger.info(\"  Num orig examples = %d\", len(eval_examples))\n",
    "    logger.info(\"  Num split examples = %d\", len(eval_features))\n",
    "    logger.info(\"  Batch size = %d\", args.predict_batch_size)\n",
    "\n",
    "    all_input_ids = torch.tensor([f.input_ids for f in eval_features], dtype=torch.long)\n",
    "    all_input_mask = torch.tensor([f.input_mask for f in eval_features], dtype=torch.long)\n",
    "    all_segment_ids = torch.tensor([f.segment_ids for f in eval_features], dtype=torch.long)\n",
    "    all_example_index = torch.arange(all_input_ids.size(0), dtype=torch.long)\n",
    "    eval_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_example_index)\n",
    "    # Run prediction for full data\n",
    "    eval_sampler = SequentialSampler(eval_data)\n",
    "    eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=args.predict_batch_size)\n",
    "\n",
    "    model.eval()\n",
    "    all_results = []\n",
    "    logger.info(\"Start evaluating\")\n",
    "    for input_ids, input_mask, segment_ids, example_indices in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "        input_ids = input_ids.to(device)\n",
    "        input_mask = input_mask.to(device)\n",
    "        segment_ids = segment_ids.to(device)\n",
    "        with torch.no_grad():\n",
    "             batch_start_logits, batch_end_logits, _ = model(input_ids, segment_ids, input_mask)\n",
    "        for i, example_index in enumerate(example_indices):\n",
    "            start_logits = batch_start_logits[i].detach().cpu().tolist()\n",
    "            end_logits = batch_end_logits[i].detach().cpu().tolist()\n",
    "            eval_feature = eval_features[example_index.item()]\n",
    "            unique_id = int(eval_feature.unique_id)\n",
    "            all_results.append(RawResult(unique_id=unique_id,\n",
    "                start_logits=start_logits,\n",
    "                end_logits=end_logits))\n",
    "\n",
    "    if global_step:\n",
    "        prediction_file_name = 'predictions_' + str(global_step) + f'_{datetime.datetime.now().strftime(\"%Y%m%d_%H:%M:%S\")}.json'\n",
    "        nbest_file_name = 'nbest_predictions_' + str(global_step) + f'_{datetime.datetime.now().strftime(\"%Y%m%d_%H:%M:%S\")}.json'\n",
    "        output_prediction_file = os.path.join(args.output_dir, prediction_file_name)\n",
    "        output_nbest_file = os.path.join(args.output_dir, nbest_file_name)\n",
    "    else:\n",
    "        output_prediction_file = os.path.join(args.output_dir, f'predictions_{datetime.datetime.now().strftime(\"%Y%m%d_%H:%M:%S\")}.json')\n",
    "        output_nbest_file = os.path.join(args.output_dir, f'nbest_predictions_{datetime.datetime.now().strftime(\"%Y%m%d_%H:%M:%S\")}.json')\n",
    "    all_predictions, all_probs, all_indices = write_predictions(args, eval_examples, eval_features, all_results,\n",
    "        args.n_best_size, args.max_answer_length,\n",
    "        args.do_lower_case, output_prediction_file,\n",
    "        output_nbest_file, args.verbose_logging, logger, args.output_prediction)\n",
    "    if generate_label:\n",
    "        return keep_high_prob_samples(all_probs, eval_features, generate_prob_th, removed_feature_index, all_indices,\n",
    "                keep_generated=args.keep_previous_generated)\n",
    "    else:\n",
    "        acc, f1 = evaluate_acc_and_f1(all_predictions, eval_examples, logger)\n",
    "        logger.info('Current EM is %.5f, F1 is %.5f', acc, f1)\n",
    "        if not (best_f1 is None or best_acc is None):\n",
    "            best_acc, best_f1 = compare_performance(args, best_acc, best_f1, acc, f1, model, logger)\n",
    "            return best_acc, best_f1\n",
    "        else:\n",
    "            return acc, f1\n",
    "\n",
    "\n",
    "def generate_self_training_samples(args, train_examples, train_features, device, model, removed_feature_index,\n",
    "        new_generated_train_features, generate_prob_th, logger):\n",
    "    '''\n",
    "    타겟 도메인 데이터에서 pseudo-label 생성\n",
    "    '''\n",
    "    logger.info('***** Generating training data for this epoch *****')\n",
    "    if args.keep_previous_generated:\n",
    "        train_features_removed_previous = []\n",
    "        for index in range(len(train_features)):\n",
    "            if index not in removed_feature_index:\n",
    "                train_features_removed_previous.append(train_features[index])\n",
    "    else:\n",
    "        train_features_removed_previous = train_features\n",
    "    cur_train_features, removed_feature_index = \\\n",
    "        evaluation_stage(args, train_examples, train_features_removed_previous, device, model, logger,\n",
    "            removed_feature_index=removed_feature_index, generate_label=True, generate_prob_th=generate_prob_th)\n",
    "    if len(cur_train_features) == 0:\n",
    "        logger.info(\"  No new training samples were generated, training procedure ends\")\n",
    "        return None, None\n",
    "    if args.keep_previous_generated:\n",
    "        new_generated_train_features.extend(cur_train_features)\n",
    "    else:\n",
    "        new_generated_train_features = cur_train_features\n",
    "    return new_generated_train_features, removed_feature_index\n",
    "\n",
    "\n",
    "def get_bert_model_parameters(model):\n",
    "    '''\n",
    "    역할:BERT optimizer 파라미터 그룹 생성 (weight decay 적용/미적용 분리)\n",
    "    반환: optimizer_grouped_parameters\n",
    "    '''\n",
    "    # Prepare optimizer\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "\n",
    "    # hack to remove pooler, which is not used\n",
    "    # thus it produce None grad that break apex\n",
    "    param_optimizer = [n for n in param_optimizer if 'pooler' not in n[0]]\n",
    "\n",
    "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "    return optimizer_grouped_parameters\n",
    "\n",
    "\n",
    "def comb_adversarial_training_stage(args, target_train_features, target_train_examples, source_train_features,\n",
    "            eval_features, eval_examples, removed_feature_index, new_generated_train_features, model, epoch,\n",
    "            device, best_acc, best_f1, logger):\n",
    "    '''\n",
    "    1. Self-training: 타겟 데이터에서 pseudo-label 생성\n",
    "    2. Question Type Probability 기반 Source-Target 매칭:\n",
    "        타겟 배치의 각 샘플에서 q_type_prob 상위 3개를 추출하여\n",
    "        각 타입에 해당하는 소스 샘플을 샘플링 (타겟 1개 → 소스 3개)\n",
    "    3. Combined Training: 타겟(pseudo) + 소스(labeled) 데이터를 concat하여 학습\n",
    "    '''\n",
    "\n",
    "    def sample_source_batch_top3(source_data, source_q_type_dict, sample_pointer, q_type_probs):\n",
    "        \"\"\"\n",
    "        q_type_probs 기반 상위 3개 타입에서 소스 샘플링\n",
    "        Args:\n",
    "            q_type_probs: [batch_size, 6] 텐서\n",
    "        Returns:\n",
    "            타겟 배치의 최대 3배 크기 소스 배치\n",
    "        \"\"\"\n",
    "        output_idx = []\n",
    "\n",
    "        for q_type_prob in q_type_probs:  # 각 타겟 샘플\n",
    "            # 상위 3개 q_type 인덱스 추출\n",
    "            top3_indices = torch.topk(q_type_prob, k=3).indices.tolist()\n",
    "\n",
    "            for q_type in top3_indices:\n",
    "                # 해당 타입에 소스 샘플이 없으면 스킵\n",
    "                if len(source_q_type_dict[q_type]) == 0:\n",
    "                    continue\n",
    "\n",
    "                next_q_idx = sample_pointer[q_type] % len(source_q_type_dict[q_type])\n",
    "                output_idx.append(source_q_type_dict[q_type][next_q_idx])\n",
    "                sample_pointer[q_type] += 1\n",
    "\n",
    "        input_ids_source, input_masks_source, segment_ids_source, start_positions_source, end_positions_source, \\\n",
    "            q_types_source = [], [], [], [], [], []\n",
    "        for idx in output_idx:\n",
    "            input_ids_source.append(source_data[idx][0].unsqueeze(0))\n",
    "            input_masks_source.append(source_data[idx][1].unsqueeze(0))\n",
    "            segment_ids_source.append(source_data[idx][2].unsqueeze(0))\n",
    "            start_positions_source.append(source_data[idx][3].unsqueeze(0))\n",
    "            end_positions_source.append(source_data[idx][4].unsqueeze(0))\n",
    "            q_types_source.append(source_data[idx][5].unsqueeze(0))\n",
    "\n",
    "        return torch.vstack(input_ids_source), torch.vstack(input_masks_source), torch.vstack(segment_ids_source), \\\n",
    "            torch.cat(start_positions_source, -1), torch.cat(end_positions_source, -1), torch.cat(q_types_source, -1)\n",
    "\n",
    "    # Generate self-training samples\n",
    "    # 1. Pseudo-label 생성\n",
    "    new_generated_train_features, removed_feature_index = generate_self_training_samples(args, target_train_examples,\n",
    "        target_train_features, device, model, removed_feature_index, new_generated_train_features, args.generate_prob_th,\n",
    "        logger)\n",
    "    if new_generated_train_features is None:\n",
    "        sys.exit()\n",
    "    \n",
    "    logger.info('\\n')\n",
    "    logger.info('====================  Start Adversarial Training Stage  ====================')\n",
    "    \n",
    "    # q_type_prob 추출 (데이터에서 가져오기)\n",
    "    all_q_type_probs = []\n",
    "    for f in new_generated_train_features:\n",
    "        # InputFeatures에 q_type_prob가 있는지 확인\n",
    "        if hasattr(f, 'q_type_prob') and f.q_type_prob is not None:\n",
    "            all_q_type_probs.append(f.q_type_prob)\n",
    "        else:\n",
    "            # q_type_prob가 없으면 one-hot 인코딩 사용\n",
    "            prob = [0.0] * 6\n",
    "            prob[f.q_type] = 1.0\n",
    "            all_q_type_probs.append(prob)\n",
    "    \n",
    "    all_input_ids = torch.tensor([f.input_ids for f in new_generated_train_features], dtype=torch.long)\n",
    "    all_input_mask = torch.tensor([f.input_mask for f in new_generated_train_features], dtype=torch.long)\n",
    "    all_segment_ids = torch.tensor([f.segment_ids for f in new_generated_train_features], dtype=torch.long)\n",
    "    all_start_positions = torch.tensor([f.start_position for f in new_generated_train_features], dtype=torch.long)\n",
    "    all_end_positions = torch.tensor([f.end_position for f in new_generated_train_features], dtype=torch.long)\n",
    "    all_q_types = torch.tensor([f.q_type for f in new_generated_train_features], dtype=torch.long)\n",
    "    all_q_type_probs = torch.tensor(all_q_type_probs, dtype=torch.float)\n",
    "    \n",
    "    train_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids,\n",
    "        all_start_positions, all_end_positions, all_q_types, all_q_type_probs)\n",
    "    \n",
    "    source_input_ids = torch.tensor([f.input_ids for f in source_train_features], dtype=torch.long)\n",
    "    source_input_mask = torch.tensor([f.input_mask for f in source_train_features], dtype=torch.long)\n",
    "    source_segment_ids = torch.tensor([f.segment_ids for f in source_train_features], dtype=torch.long)\n",
    "    source_start_positions = torch.tensor([f.start_position for f in source_train_features], dtype=torch.long)\n",
    "    source_end_positions = torch.tensor([f.end_position for f in source_train_features], dtype=torch.long)\n",
    "    source_q_types = []\n",
    "    source_q_type_dict = {\n",
    "        0: [],\n",
    "        1: [],\n",
    "        2: [],\n",
    "        3: [],\n",
    "        4: [],\n",
    "        5: []\n",
    "    }\n",
    "    for idx, f in enumerate(source_train_features):\n",
    "        source_q_types.append(f.q_type)\n",
    "        source_q_type_dict[f.q_type].append(idx)\n",
    "    source_q_types = torch.tensor(source_q_types, dtype=torch.long)\n",
    "    for key in source_q_type_dict.keys():\n",
    "        random.shuffle(source_q_type_dict[key])\n",
    "    sample_pointer = [0] * 6\n",
    "    source_data = TensorDataset(source_input_ids, source_input_mask, source_segment_ids, source_start_positions, \n",
    "        source_end_positions, source_q_types)\n",
    "\n",
    "    train_sampler = BERTRandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=args.train_batch_size)\n",
    "    \n",
    "    data_len = len(new_generated_train_features)\n",
    "    logger.info(\"  Num split examples = %d\", data_len)\n",
    "    logger.info(\"  Batch size = %d\", args.train_batch_size)\n",
    "    num_train_steps = math.ceil(data_len / args.train_batch_size / args.gradient_accumulation_steps)\n",
    "    if num_train_steps == 0 and data_len > 0:\n",
    "        num_train_steps = 1\n",
    "    t_total = num_train_steps\n",
    "    logger.info(\"  Num steps = %d\", num_train_steps)\n",
    "\n",
    "    loss_sum = 0\n",
    "    optimizer_grouped_parameters = get_bert_model_parameters(model)\n",
    "    optimizer = BertAdam(optimizer_grouped_parameters,\n",
    "            lr=args.adapt_learning_rate,\n",
    "            warmup=args.warmup_proportion,\n",
    "            t_total=t_total)\n",
    "    global_step = 0\n",
    "\n",
    "    # 타겟 배치 순회(pseudo-labeling->소스 매칭->결합 학습)\n",
    "    for step, batch in enumerate(tqdm(train_dataloader, desc=\"Iteration\")):\n",
    "        model.train()\n",
    "        # 소스 배치 샘플링 (question type probability 기반 상위 3개)\n",
    "        batch_source = sample_source_batch_top3(source_data, source_q_type_dict, sample_pointer, batch[-1])\n",
    "        batch = tuple(t.to(device) for t in batch[:-1])  # q_type_probs는 제외\n",
    "        batch_source = tuple(t.to(device) for t in batch_source)\n",
    "        input_ids, input_masks, segment_ids, start_positions, end_positions, q_types = batch\n",
    "        input_ids_source, input_masks_source, segment_ids_source, start_positions_source, end_positions_source, q_types_source = batch_source\n",
    "        # 타겟 + 소스 concat\n",
    "        input_ids = torch.cat((input_ids, input_ids_source), 0)\n",
    "        input_masks = torch.cat((input_masks, input_masks_source), 0)\n",
    "        segment_ids = torch.cat((segment_ids, segment_ids_source), 0)\n",
    "        start_positions = torch.cat((start_positions, start_positions_source), 0)\n",
    "        end_positions = torch.cat((end_positions, end_positions_source), 0)\n",
    "        q_types = torch.cat((q_types, q_types_source), 0)\n",
    "        # QC4QA loss 계산 및 역전파\n",
    "        loss = model.forward_ours(input_ids, segment_ids, input_masks, start_positions,\n",
    "                end_positions, q_types, lambda_c=args.lambda_c)\n",
    "        if args.gradient_accumulation_steps > 1:\n",
    "            loss = loss / args.gradient_accumulation_steps\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        if (step + 1) % args.gradient_accumulation_steps == 0:\n",
    "            # modify learning rate with special warm up BERT uses\n",
    "            lr_this_step = args.adapt_learning_rate * warmup_linear(global_step / t_total, args.warmup_proportion)\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr_this_step\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            global_step += 1\n",
    "        \n",
    "    final_acc, final_f1 = None, None\n",
    "    if epoch == args.num_train_epochs - 1:\n",
    "        final_acc, final_f1 = evaluation_stage(args, eval_examples, eval_features, device, model,\n",
    "            global_step=global_step, best_acc=None, best_f1=None, logger=logger)\n",
    "        best_acc, best_f1 = compare_performance(args, best_acc, best_f1, final_acc, final_f1, model, logger)\n",
    "    else:\n",
    "        best_acc, best_f1 = evaluation_stage(args, eval_examples, eval_features, device, model,\n",
    "            global_step=global_step, best_acc=best_acc, best_f1=best_f1, logger=logger)\n",
    "    return best_acc, best_f1, final_acc, final_f1\n",
    "\n",
    "\n",
    "def prepare_model(args, device):\n",
    "    # Source 도메인에서 fine-tuned 모델 로드(run_source.py는 사전학습 모델 로드)\n",
    "    input_model_file = os.path.join(args.input_dir, args.input_model_file)\n",
    "    model_state_dict = torch.load(input_model_file)\n",
    "    model = BertForQuestionAnsweringQC4QA.from_pretrained(args.bert_model, state_dict=model_state_dict, args=args)\n",
    "    model.to(device)\n",
    "    return model\n",
    "\n",
    "\n",
    "def adaptation_stage(args, tokenizer, device, logger, debug=True):\n",
    "    '''\n",
    "    역할: 도메인 적응 메인 루프\n",
    "    '''\n",
    "    ### 데이터 수 조절 ### \n",
    "    sample_limit = 100\n",
    "    \n",
    "    model = prepare_model(args, device)\n",
    "    best_acc, best_f1 = 0, 0\n",
    "    \n",
    "    read_limit = None\n",
    "    if debug:\n",
    "        read_limit = 50\n",
    "\n",
    "    ## Read target training examples\n",
    "    logger.info(\"***** Reading Target Unlabeled Training Samples *****\")\n",
    "    train_features, train_examples = read_features_and_examples(args, args.target_train_file, tokenizer, logger,\n",
    "        use_simple_feature=False, read_examples=True, limit=read_limit)\n",
    "\n",
    "    ## Read source training examples\n",
    "    logger.info(\"***** Reading Source Training Samples *****\")\n",
    "    source_train_features, _ = read_features_and_examples(args, args.source_train_file, tokenizer, logger,\n",
    "        use_simple_feature=False, read_examples=True, limit=read_limit)\n",
    "\n",
    "    # Read evaluation samples\n",
    "    logger.info(\"***** Reading Evaluation Samples *****\")\n",
    "    eval_features, eval_examples = read_features_and_examples(args, args.target_predict_file, tokenizer, logger,\n",
    "        use_simple_feature=False, read_examples=True, limit=read_limit)\n",
    "\n",
    "    removed_feature_index = set()\n",
    "    new_generated_train_features = []\n",
    "    final_acc, final_f1 = 0.0, 0.0\n",
    "    for epoch in trange(int(args.num_train_epochs), desc=\"Epoch\"):\n",
    "        logger.info('\\n')\n",
    "        logger.info(' ###########  Start Training Epoch %d  ###########', epoch + 1)\n",
    "        logger.info('\\n')\n",
    "        best_acc, best_f1, final_acc, final_f1 = comb_adversarial_training_stage(args, train_features, train_examples,\n",
    "                source_train_features, eval_features, eval_examples, removed_feature_index, new_generated_train_features,\n",
    "                model, epoch, device, best_acc, best_f1, logger)\n",
    "        logger.info('\\n')\n",
    "        logger.info(' ###########  End Training Epoch %d  ###########', epoch + 1)\n",
    "        logger.info('\\n')\n",
    "\n",
    "    # Save the final trained model\n",
    "    model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n",
    "    output_model_file = os.path.join(args.output_dir, args.output_model_file + '.final')\n",
    "    torch.save(model_to_save.state_dict(), output_model_file)\n",
    "    logger.info('The final model has been save')\n",
    "    logger.info('*** The Training Stage is Ended ***')\n",
    "    logger.info('\\n\\nBest EM is %.5f. Best F1 is %.5f', best_acc, best_f1)\n",
    "    logger.info('\\n\\nFinal EM is %.5f. Best F1 is %.5f', final_acc, final_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b504532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# argument parsing\n",
    "args = argparse.Namespace(\n",
    "    debug = True, # sample_limit 사이즈로 디버깅\n",
    "    bert_model='bert-base-uncased',\n",
    "    do_adaptation=True,\n",
    "    do_predict=False,\n",
    "    do_lower_case=True,\n",
    "    source_train_file=\"../../data/squad/train-v1.1_classified_qtype_prob.jsonl\",\n",
    "    target_train_file=\"../../data/cnn/cnn_train_classified_qtype_prob.jsonl\",\n",
    "    target_predict_file=\"../../data/cnn/cnn_dev.json\",\n",
    "    input_dir=\"../../model/qa/squad\",\n",
    "    input_model_file=\"best_model_0916.bin\",\n",
    "    output_dir=\"../../model/qa/squad2target\",\n",
    "    output_model_file=\"adaptation_20251105_1300.bin\",\n",
    "    logger_path=\"../../../model/qa/squad2target\",\n",
    "    max_seq_length=512,\n",
    "    seed=42,\n",
    "    gradient_accumulation_steps=1,\n",
    "    train_batch_size=12,\n",
    "    predict_batch_size=12,\n",
    "    num_workers=4,\n",
    "    evaluation_interval=2000,\n",
    "    loss_logging_interval=500,\n",
    "    train_learning_rate=3e-5,\n",
    "    num_train_epochs=2,\n",
    "    warmup_proportion=0.1,\n",
    "    n_best_size=20,\n",
    "    max_answer_length=30,\n",
    "    verbose_logging=False,\n",
    "    use_simple_feature=False,\n",
    "    generate_prob_th=0.6,\n",
    "    keep_previous_generated=False,\n",
    "    use_BN=True,\n",
    "    output_prediction=True,\n",
    "    source_sampling_ratio=3,\n",
    "    doc_stride=128,\n",
    "    max_query_length=64,\n",
    "    adapt_learning_rate=1e-5,\n",
    "    lambda_c=0.1,\n",
    "    sample_limit= 100  ### 디버깅용 데이터셋 크기\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9468b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/05/2025 12:25:53 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/gayeon44/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device = cuda\n"
     ]
    }
   ],
   "source": [
    "# 3) 로거/디바이스/토크나이저 준비\n",
    "logger = config_logger(args.logger_path)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = BertTokenizer.from_pretrained(args.bert_model, do_lower_case=args.do_lower_case)\n",
    "print(f\"device = {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45e6e9d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/05/2025 12:26:01 - INFO - pytorch_pretrained_bert.modeling -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /home/gayeon44/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
      "11/05/2025 12:26:01 - INFO - pytorch_pretrained_bert.modeling -   extracting archive file /home/gayeon44/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /tmp/tmpn60klqn6\n",
      "11/05/2025 12:26:03 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_seq_length\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_BN\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "11/05/2025 12:26:05 - INFO - pytorch_pretrained_bert.modeling -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /home/gayeon44/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
      "11/05/2025 12:26:05 - INFO - pytorch_pretrained_bert.modeling -   extracting archive file /home/gayeon44/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /tmp/tmprjpqet5y\n",
      "11/05/2025 12:26:06 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_seq_length\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_BN\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "11/05/2025 12:26:07 - INFO - utils.ConfigLogger -   ***** Reading Target Unlabeled Training Samples *****\n",
      "11/05/2025 12:27:08 - INFO - utils.ConfigLogger -   ***** Reading Source Training Samples *****\n",
      "11/05/2025 12:27:15 - INFO - utils.ConfigLogger -   ***** Reading Evaluation Samples *****\n",
      "Epoch:   0%|          | 0/2 [00:00<?, ?it/s]11/05/2025 12:27:17 - INFO - utils.ConfigLogger -   \n",
      "\n",
      "11/05/2025 12:27:17 - INFO - utils.ConfigLogger -    ###########  Start Training Epoch 1  ###########\n",
      "11/05/2025 12:27:17 - INFO - utils.ConfigLogger -   \n",
      "\n",
      "11/05/2025 12:27:17 - INFO - utils.ConfigLogger -   ***** Generating training data for this epoch *****\n",
      "11/05/2025 12:27:17 - INFO - utils.ConfigLogger -   ***** Running Evaluation Stage *****\n",
      "11/05/2025 12:27:17 - INFO - utils.ConfigLogger -     Num orig examples = 50\n",
      "11/05/2025 12:27:17 - INFO - utils.ConfigLogger -     Num split examples = 50\n",
      "11/05/2025 12:27:17 - INFO - utils.ConfigLogger -     Batch size = 12\n",
      "11/05/2025 12:27:17 - INFO - utils.ConfigLogger -   Start evaluating\n",
      "Evaluating: 100%|██████████| 5/5 [00:00<00:00, 15.55it/s]\n",
      "11/05/2025 12:27:17 - INFO - utils.ConfigLogger -   Writing predictions to: model/squad2target/predictions_20251105_12:27:17.json\n",
      "11/05/2025 12:27:17 - INFO - utils.ConfigLogger -   Writing nbest to: model/squad2target/nbest_predictions_20251105_12:27:17.json\n",
      "11/05/2025 12:27:17 - INFO - utils.ConfigLogger -   \n",
      "\n",
      "11/05/2025 12:27:17 - INFO - utils.ConfigLogger -   ====================  Start Adversarial Training Stage  ====================\n",
      "11/05/2025 12:27:17 - INFO - utils.ConfigLogger -     Num split examples = 2\n",
      "11/05/2025 12:27:17 - INFO - utils.ConfigLogger -     Batch size = 12\n",
      "11/05/2025 12:27:17 - INFO - utils.ConfigLogger -     Num steps = 1\n",
      "/home/gayeon44/gayeon/[DA]/preliminary/QC4QA/src/pytorch_pretrained_bert/optimization.py:132: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha = 1) (Triggered internally at /pytorch/torch/csrc/utils/python_arg_parser.cpp:1805.)\n",
      "  next_m.mul_(beta1).add_(1 - beta1, grad)\n",
      "Iteration: 100%|██████████| 1/1 [00:00<00:00,  3.55it/s]\n",
      "11/05/2025 12:27:18 - INFO - utils.ConfigLogger -   ***** Running Predictions *****\n",
      "11/05/2025 12:27:18 - INFO - utils.ConfigLogger -     Num orig examples = 50\n",
      "11/05/2025 12:27:18 - INFO - utils.ConfigLogger -     Num split examples = 50\n",
      "11/05/2025 12:27:18 - INFO - utils.ConfigLogger -     Batch size = 12\n",
      "11/05/2025 12:27:18 - INFO - utils.ConfigLogger -   Start evaluating\n",
      "Evaluating: 100%|██████████| 5/5 [00:00<00:00, 30.07it/s]\n",
      "11/05/2025 12:27:19 - INFO - utils.ConfigLogger -   Writing predictions to: model/squad2target/predictions_1_20251105_12:27:19.json\n",
      "11/05/2025 12:27:19 - INFO - utils.ConfigLogger -   Writing nbest to: model/squad2target/nbest_predictions_1_20251105_12:27:19.json\n",
      "/tmp/ipykernel_9481/3773295735.py:27: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead\n",
      "  logger.warn(message)\n",
      "11/05/2025 12:27:19 - WARNING - utils.ConfigLogger -   Unanswered question validation/fd0dc9fcbb41450c388f6d3f90153da19f53e050 will receive score 0.\n",
      "11/05/2025 12:27:19 - WARNING - utils.ConfigLogger -   Unanswered question validation/7ad6bf269adef016560ea4d857150771842e0d23 will receive score 0.\n",
      "11/05/2025 12:27:19 - WARNING - utils.ConfigLogger -   Unanswered question validation/de308d2ec5722143d8156c3fe2192a2884ca86f9 will receive score 0.\n",
      "11/05/2025 12:27:19 - WARNING - utils.ConfigLogger -   Unanswered question validation/c1875aa38c8ac079d128a4ddf92259479955abcc will receive score 0.\n",
      "11/05/2025 12:27:19 - WARNING - utils.ConfigLogger -   Unanswered question validation/1993fab60212e6627aea8ae78a33d9a81ab0e27c will receive score 0.\n",
      "11/05/2025 12:27:19 - WARNING - utils.ConfigLogger -   Unanswered question validation/6253f6fb35a222d648643465c6b9399dcdbe13a1 will receive score 0.\n",
      "11/05/2025 12:27:19 - WARNING - utils.ConfigLogger -   Unanswered question validation/1714ebeffea86e2788bfb7653ada360f61af2ff7 will receive score 0.\n",
      "11/05/2025 12:27:19 - WARNING - utils.ConfigLogger -   Unanswered question validation/656544054260257c0fa082e68c795a2152a49fc8 will receive score 0.\n",
      "11/05/2025 12:27:19 - WARNING - utils.ConfigLogger -   Unanswered question validation/d4c01b8a4fb735a1734d9c8fadddb1028b751b34 will receive score 0.\n",
      "11/05/2025 12:27:19 - WARNING - utils.ConfigLogger -   Unanswered question validation/6c22ba2fa5c59946e284a888fcc7c74fcac38e65 will receive score 0.\n",
      "11/05/2025 12:27:19 - WARNING - utils.ConfigLogger -   Unanswered question validation/bf9c95d342a51ffe32440116c9d5b1b7dfa96531 will receive score 0.\n",
      "11/05/2025 12:27:19 - WARNING - utils.ConfigLogger -   Unanswered question validation/d787ad7de05d3f63168579a29ded97b35bffd1b6 will receive score 0.\n",
      "11/05/2025 12:27:19 - WARNING - utils.ConfigLogger -   Unanswered question validation/c9d2e46b29b21b8b598b4b80803c1af651e7f1d2 will receive score 0.\n",
      "11/05/2025 12:27:19 - WARNING - utils.ConfigLogger -   Unanswered question validation/0957a9a140132d7f2ba596b62628fb98e5470a80 will receive score 0.\n",
      "11/05/2025 12:27:19 - WARNING - utils.ConfigLogger -   Unanswered question validation/033daf6ae73daf122e5982c6422b284d69b42764 will receive score 0.\n",
      "11/05/2025 12:27:19 - WARNING - utils.ConfigLogger -   Unanswered question validation/b3eff20f10ae157008ccf8962ed340feee33d31b will receive score 0.\n",
      "11/05/2025 12:27:19 - WARNING - utils.ConfigLogger -   Unanswered question validation/b1096769370114b16f44468ecb9aa2f9addaec3c will receive score 0.\n",
      "11/05/2025 12:27:19 - WARNING - utils.ConfigLogger -   Unanswered question validation/cbd9f3fae1ca0c7eabf3d38b3311c9998995d13e will receive score 0.\n",
      "11/05/2025 12:27:19 - WARNING - utils.ConfigLogger -   Unanswered question validation/01f460fa9d4289827ece2af1344485728e414981 will receive score 0.\n",
      "11/05/2025 12:27:19 - WARNING - utils.ConfigLogger -   Unanswered question validation/78c8d1e634e80e210626dc12671d13b62d10bc99 will receive score 0.\n",
      "11/05/2025 12:27:19 - WARNING - utils.ConfigLogger -   Unanswered question validation/5ec7c78400e01b1dcf2bd7d2cf9d6610ff59af12 will receive score 0.\n",
      "11/05/2025 12:27:19 - INFO - utils.ConfigLogger -   Current EM is 13.79310, F1 is 24.43514\n",
      "11/05/2025 12:27:19 - INFO - utils.ConfigLogger -   Current model BEATS previous best model, previous best is EM = 0.00000, F1 = 0.00000\n",
      "11/05/2025 12:27:19 - INFO - utils.ConfigLogger -   Current best model has been saved!\n",
      "11/05/2025 12:27:19 - INFO - utils.ConfigLogger -   \n",
      "\n",
      "11/05/2025 12:27:19 - INFO - utils.ConfigLogger -    ###########  End Training Epoch 1  ###########\n",
      "11/05/2025 12:27:19 - INFO - utils.ConfigLogger -   \n",
      "\n",
      "Epoch:  50%|█████     | 1/2 [00:01<00:01,  1.86s/it]11/05/2025 12:27:19 - INFO - utils.ConfigLogger -   \n",
      "\n",
      "11/05/2025 12:27:19 - INFO - utils.ConfigLogger -    ###########  Start Training Epoch 2  ###########\n",
      "11/05/2025 12:27:19 - INFO - utils.ConfigLogger -   \n",
      "\n",
      "11/05/2025 12:27:19 - INFO - utils.ConfigLogger -   ***** Generating training data for this epoch *****\n",
      "11/05/2025 12:27:19 - INFO - utils.ConfigLogger -   ***** Running Evaluation Stage *****\n",
      "11/05/2025 12:27:19 - INFO - utils.ConfigLogger -     Num orig examples = 50\n",
      "11/05/2025 12:27:19 - INFO - utils.ConfigLogger -     Num split examples = 50\n",
      "11/05/2025 12:27:19 - INFO - utils.ConfigLogger -     Batch size = 12\n",
      "11/05/2025 12:27:19 - INFO - utils.ConfigLogger -   Start evaluating\n",
      "Evaluating: 100%|██████████| 5/5 [00:00<00:00, 29.85it/s]\n",
      "11/05/2025 12:27:19 - INFO - utils.ConfigLogger -   Writing predictions to: model/squad2target/predictions_20251105_12:27:19.json\n",
      "11/05/2025 12:27:19 - INFO - utils.ConfigLogger -   Writing nbest to: model/squad2target/nbest_predictions_20251105_12:27:19.json\n",
      "11/05/2025 12:27:19 - INFO - utils.ConfigLogger -   \n",
      "\n",
      "11/05/2025 12:27:19 - INFO - utils.ConfigLogger -   ====================  Start Adversarial Training Stage  ====================\n",
      "11/05/2025 12:27:19 - INFO - utils.ConfigLogger -     Num split examples = 2\n",
      "11/05/2025 12:27:19 - INFO - utils.ConfigLogger -     Batch size = 12\n",
      "11/05/2025 12:27:19 - INFO - utils.ConfigLogger -     Num steps = 1\n",
      "Iteration: 100%|██████████| 1/1 [00:00<00:00, 10.83it/s]\n",
      "11/05/2025 12:27:19 - INFO - utils.ConfigLogger -   ***** Running Predictions *****\n",
      "11/05/2025 12:27:19 - INFO - utils.ConfigLogger -     Num orig examples = 50\n",
      "11/05/2025 12:27:19 - INFO - utils.ConfigLogger -     Num split examples = 50\n",
      "11/05/2025 12:27:19 - INFO - utils.ConfigLogger -     Batch size = 12\n",
      "11/05/2025 12:27:19 - INFO - utils.ConfigLogger -   Start evaluating\n",
      "Evaluating: 100%|██████████| 5/5 [00:00<00:00, 30.02it/s]\n",
      "11/05/2025 12:27:19 - INFO - utils.ConfigLogger -   Writing predictions to: model/squad2target/predictions_1_20251105_12:27:19.json\n",
      "11/05/2025 12:27:19 - INFO - utils.ConfigLogger -   Writing nbest to: model/squad2target/nbest_predictions_1_20251105_12:27:19.json\n",
      "11/05/2025 12:27:19 - WARNING - utils.ConfigLogger -   Unanswered question validation/fd0dc9fcbb41450c388f6d3f90153da19f53e050 will receive score 0.\n",
      "11/05/2025 12:27:19 - WARNING - utils.ConfigLogger -   Unanswered question validation/7ad6bf269adef016560ea4d857150771842e0d23 will receive score 0.\n",
      "11/05/2025 12:27:19 - WARNING - utils.ConfigLogger -   Unanswered question validation/de308d2ec5722143d8156c3fe2192a2884ca86f9 will receive score 0.\n",
      "11/05/2025 12:27:19 - WARNING - utils.ConfigLogger -   Unanswered question validation/c1875aa38c8ac079d128a4ddf92259479955abcc will receive score 0.\n",
      "11/05/2025 12:27:19 - WARNING - utils.ConfigLogger -   Unanswered question validation/1993fab60212e6627aea8ae78a33d9a81ab0e27c will receive score 0.\n",
      "11/05/2025 12:27:19 - WARNING - utils.ConfigLogger -   Unanswered question validation/6253f6fb35a222d648643465c6b9399dcdbe13a1 will receive score 0.\n",
      "11/05/2025 12:27:19 - WARNING - utils.ConfigLogger -   Unanswered question validation/1714ebeffea86e2788bfb7653ada360f61af2ff7 will receive score 0.\n",
      "11/05/2025 12:27:19 - WARNING - utils.ConfigLogger -   Unanswered question validation/656544054260257c0fa082e68c795a2152a49fc8 will receive score 0.\n",
      "11/05/2025 12:27:19 - WARNING - utils.ConfigLogger -   Unanswered question validation/d4c01b8a4fb735a1734d9c8fadddb1028b751b34 will receive score 0.\n",
      "11/05/2025 12:27:19 - WARNING - utils.ConfigLogger -   Unanswered question validation/6c22ba2fa5c59946e284a888fcc7c74fcac38e65 will receive score 0.\n",
      "11/05/2025 12:27:19 - WARNING - utils.ConfigLogger -   Unanswered question validation/bf9c95d342a51ffe32440116c9d5b1b7dfa96531 will receive score 0.\n",
      "11/05/2025 12:27:19 - WARNING - utils.ConfigLogger -   Unanswered question validation/d787ad7de05d3f63168579a29ded97b35bffd1b6 will receive score 0.\n",
      "11/05/2025 12:27:19 - WARNING - utils.ConfigLogger -   Unanswered question validation/c9d2e46b29b21b8b598b4b80803c1af651e7f1d2 will receive score 0.\n",
      "11/05/2025 12:27:19 - WARNING - utils.ConfigLogger -   Unanswered question validation/0957a9a140132d7f2ba596b62628fb98e5470a80 will receive score 0.\n",
      "11/05/2025 12:27:19 - WARNING - utils.ConfigLogger -   Unanswered question validation/033daf6ae73daf122e5982c6422b284d69b42764 will receive score 0.\n",
      "11/05/2025 12:27:19 - WARNING - utils.ConfigLogger -   Unanswered question validation/b3eff20f10ae157008ccf8962ed340feee33d31b will receive score 0.\n",
      "11/05/2025 12:27:19 - WARNING - utils.ConfigLogger -   Unanswered question validation/b1096769370114b16f44468ecb9aa2f9addaec3c will receive score 0.\n",
      "11/05/2025 12:27:19 - WARNING - utils.ConfigLogger -   Unanswered question validation/cbd9f3fae1ca0c7eabf3d38b3311c9998995d13e will receive score 0.\n",
      "11/05/2025 12:27:19 - WARNING - utils.ConfigLogger -   Unanswered question validation/01f460fa9d4289827ece2af1344485728e414981 will receive score 0.\n",
      "11/05/2025 12:27:19 - WARNING - utils.ConfigLogger -   Unanswered question validation/78c8d1e634e80e210626dc12671d13b62d10bc99 will receive score 0.\n",
      "11/05/2025 12:27:19 - WARNING - utils.ConfigLogger -   Unanswered question validation/5ec7c78400e01b1dcf2bd7d2cf9d6610ff59af12 will receive score 0.\n",
      "11/05/2025 12:27:19 - INFO - utils.ConfigLogger -   Current EM is 13.79310, F1 is 24.43514\n",
      "11/05/2025 12:27:19 - INFO - utils.ConfigLogger -   Current model CANNOT beat previous best model, previous best is EM = 13.79310, F1 = 24.43514\n",
      "11/05/2025 12:27:19 - INFO - utils.ConfigLogger -   \n",
      "\n",
      "11/05/2025 12:27:19 - INFO - utils.ConfigLogger -    ###########  End Training Epoch 2  ###########\n",
      "11/05/2025 12:27:19 - INFO - utils.ConfigLogger -   \n",
      "\n",
      "Epoch: 100%|██████████| 2/2 [00:02<00:00,  1.19s/it]\n",
      "11/05/2025 12:27:20 - INFO - utils.ConfigLogger -   The final model has been save\n",
      "11/05/2025 12:27:20 - INFO - utils.ConfigLogger -   *** The Training Stage is Ended ***\n",
      "11/05/2025 12:27:20 - INFO - utils.ConfigLogger -   \n",
      "\n",
      "Best EM is 13.79310. Best F1 is 24.43514\n",
      "11/05/2025 12:27:20 - INFO - utils.ConfigLogger -   \n",
      "\n",
      "Final EM is 13.79310. Best F1 is 24.43514\n"
     ]
    }
   ],
   "source": [
    "# 4) 입력 모델 로드 및 적응 단계 실행\n",
    "model = prepare_model(args, device)\n",
    "adaptation_stage(args, tokenizer, device, logger, debug=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "da",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
