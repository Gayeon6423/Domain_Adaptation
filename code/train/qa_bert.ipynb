{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3267d0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gayeon39/miniconda3/envs/da/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"NCCL_P2P_DISABLE\"] = \"1\"\n",
    "os.environ[\"NCCL_IB_DISABLE\"] = \"1\"\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from transformers import DefaultDataCollator\n",
    "from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
    "import os\n",
    "from huggingface_hub import login\n",
    "login(token=\"REDACTED_HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a6ca050",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '5733b0fb4776f41900661043',\n",
       " 'title': 'University_of_Notre_Dame',\n",
       " 'context': \"Father Joseph Carrier, C.S.C. was Director of the Science Museum and the Library and Professor of Chemistry and Physics until 1874. Carrier taught that scientific research and its promise for progress were not antagonistic to the ideals of intellectual and moral culture endorsed by the Church. One of Carrier's students was Father John Augustine Zahm (1851–1921) who was made Professor and Co-Director of the Science Department at age 23 and by 1900 was a nationally prominent scientist and naturalist. Zahm was active in the Catholic Summer School movement, which introduced Catholic laity to contemporary intellectual issues. His book Evolution and Dogma (1896) defended certain aspects of evolutionary theory as true, and argued, moreover, that even the great Church teachers Thomas Aquinas and Augustine taught something like it. The intervention of Irish American Catholics in Rome prevented Zahm's censure by the Vatican. In 1913, Zahm and former President Theodore Roosevelt embarked on a major expedition through the Amazon.\",\n",
       " 'question': 'What was the lifespan of John Augustine Zahm?',\n",
       " 'answers': {'text': ['1851–1921'], 'answer_start': [353]}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad = load_dataset(\"squad\", split=\"train[:100]\")\n",
    "squad = squad.train_test_split(test_size=0.2)\n",
    "squad_0 = squad['train'][0]\n",
    "squad_0\n",
    "# id : 질문의 고유 ID\n",
    "# title : 질문이 속한 문서의 제목\n",
    "# context : 질문에 대한 답이 포함된 문장\n",
    "# question : 질문\n",
    "# answers : 정답 정보\n",
    "#   - answer_start : 정답의 시작 위치\n",
    "#   - text : 정답의 내용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff107693",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 80/80 [00:00<00:00, 1664.40 examples/s]\n",
      "Map: 100%|██████████| 20/20 [00:00<00:00, 1560.15 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n",
    "def preprocess_function(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]] # question을 리스트로 변환\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=384,\n",
    "        truncation=\"only_second\", # 두 번째 텍스트(context)가 길어질 경우, context만 384넘으면 잘라냄\n",
    "        return_offsets_mapping=True, # 답변의 시작 위치와 끝 위치를 원래의 context에 매핑\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    answers = examples[\"answers\"]\n",
    "    return {\n",
    "        **inputs, \n",
    "        \"offset_mapping\": offset_mapping,\n",
    "        \"answers\": answers,\n",
    "    }\n",
    "\n",
    "# tokenized_dataset = squad.map(preprocess_function, \n",
    "#                               batched=True, # 여러 샘플을 한 번에 처리\n",
    "#                               remove_columns=squad[\"train\"].column_names) # train 데이터셋의 컬럼을 제거\n",
    "tokenized_dataset = squad.map(preprocess_function, \n",
    "                              batched=True) # 여러 샘플을 한 번에 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1e81b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context length: 1033\n",
      "Input IDs length: 384\n",
      "Attention mask length: 384\n",
      "Offset mapping length: 384\n",
      "Answers length: 2\n",
      "**************************************************\n",
      "Context: Father Joseph Carrier, C.S.C. was Director of the Science Museum and the Library and Professor of Chemistry and Physics until 1874. Carrier taught that scientific research and its promise for progress were not antagonistic to the ideals of intellectual and moral culture endorsed by the Church. One of Carrier's students was Father John Augustine Zahm (1851–1921) who was made Professor and Co-Director of the Science Department at age 23 and by 1900 was a nationally prominent scientist and naturalist. Zahm was active in the Catholic Summer School movement, which introduced Catholic laity to contemporary intellectual issues. His book Evolution and Dogma (1896) defended certain aspects of evolutionary theory as true, and argued, moreover, that even the great Church teachers Thomas Aquinas and Augustine taught something like it. The intervention of Irish American Catholics in Rome prevented Zahm's censure by the Vatican. In 1913, Zahm and former President Theodore Roosevelt embarked on a major expedition through the Amazon.\n",
      "Input IDs: [101, 2054, 2001, 1996, 26462, 1997, 2198, 14060, 23564, 14227, 1029, 102, 2269, 3312, 6839, 1010, 1039, 1012, 1055, 1012, 1039, 1012, 2001, 2472, 1997, 1996, 2671, 2688, 1998, 1996, 3075, 1998, 2934, 1997, 6370, 1998, 5584, 2127, 7586, 1012, 6839, 4036, 2008, 4045, 2470, 1998, 2049, 4872, 2005, 5082, 2020, 2025, 17379, 2594, 2000, 1996, 15084, 1997, 7789, 1998, 7191, 3226, 11763, 2011, 1996, 2277, 1012, 2028, 1997, 6839, 1005, 1055, 2493, 2001, 2269, 2198, 14060, 23564, 14227, 1006, 8792, 1516, 4885, 1007, 2040, 2001, 2081, 2934, 1998, 2522, 1011, 2472, 1997, 1996, 2671, 2533, 2012, 2287, 2603, 1998, 2011, 5141, 2001, 1037, 9582, 4069, 7155, 1998, 19176, 1012, 23564, 14227, 2001, 3161, 1999, 1996, 3234, 2621, 2082, 2929, 1010, 2029, 3107, 3234, 21110, 3723, 2000, 3824, 7789, 3314, 1012, 2010, 2338, 6622, 1998, 3899, 2863, 1006, 6306, 1007, 8047, 3056, 5919, 1997, 12761, 3399, 2004, 2995, 1010, 1998, 5275, 1010, 9308, 1010, 2008, 2130, 1996, 2307, 2277, 5089, 2726, 1037, 12519, 3022, 1998, 14060, 4036, 2242, 2066, 2009, 1012, 1996, 8830, 1997, 3493, 2137, 10774, 1999, 4199, 8729, 23564, 14227, 1005, 1055, 8292, 3619, 5397, 2011, 1996, 12111, 1012, 1999, 5124, 1010, 23564, 14227, 1998, 2280, 2343, 10117, 8573, 11299, 2006, 1037, 2350, 5590, 2083, 1996, 9733, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Attention mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Offset mapping: [[0, 0], [0, 4], [5, 8], [9, 12], [13, 21], [22, 24], [25, 29], [30, 39], [40, 42], [42, 44], [44, 45], [0, 0], [0, 6], [7, 13], [14, 21], [21, 22], [23, 24], [24, 25], [25, 26], [26, 27], [27, 28], [28, 29], [30, 33], [34, 42], [43, 45], [46, 49], [50, 57], [58, 64], [65, 68], [69, 72], [73, 80], [81, 84], [85, 94], [95, 97], [98, 107], [108, 111], [112, 119], [120, 125], [126, 130], [130, 131], [132, 139], [140, 146], [147, 151], [152, 162], [163, 171], [172, 175], [176, 179], [180, 187], [188, 191], [192, 200], [201, 205], [206, 209], [210, 220], [220, 222], [223, 225], [226, 229], [230, 236], [237, 239], [240, 252], [253, 256], [257, 262], [263, 270], [271, 279], [280, 282], [283, 286], [287, 293], [293, 294], [295, 298], [299, 301], [302, 309], [309, 310], [310, 311], [312, 320], [321, 324], [325, 331], [332, 336], [337, 346], [347, 349], [349, 351], [352, 353], [353, 357], [357, 358], [358, 362], [362, 363], [364, 367], [368, 371], [372, 376], [377, 386], [387, 390], [391, 393], [393, 394], [394, 402], [403, 405], [406, 409], [410, 417], [418, 428], [429, 431], [432, 435], [436, 438], [439, 442], [443, 445], [446, 450], [451, 454], [455, 456], [457, 467], [468, 477], [478, 487], [488, 491], [492, 502], [502, 503], [504, 506], [506, 508], [509, 512], [513, 519], [520, 522], [523, 526], [527, 535], [536, 542], [543, 549], [550, 558], [558, 559], [560, 565], [566, 576], [577, 585], [586, 589], [589, 591], [592, 594], [595, 607], [608, 620], [621, 627], [627, 628], [629, 632], [633, 637], [638, 647], [648, 651], [652, 655], [655, 657], [658, 659], [659, 663], [663, 664], [665, 673], [674, 681], [682, 689], [690, 692], [693, 705], [706, 712], [713, 715], [716, 720], [720, 721], [722, 725], [726, 732], [732, 733], [734, 742], [742, 743], [744, 748], [749, 753], [754, 757], [758, 763], [764, 770], [771, 779], [780, 786], [787, 788], [788, 792], [792, 794], [795, 798], [799, 808], [809, 815], [816, 825], [826, 830], [831, 833], [833, 834], [835, 838], [839, 851], [852, 854], [855, 860], [861, 869], [870, 879], [880, 882], [883, 887], [888, 897], [898, 900], [900, 902], [902, 903], [903, 904], [905, 907], [907, 909], [909, 912], [913, 915], [916, 919], [920, 927], [927, 928], [929, 931], [932, 936], [936, 937], [938, 940], [940, 942], [943, 946], [947, 953], [954, 963], [964, 972], [973, 982], [983, 991], [992, 994], [995, 996], [997, 1002], [1003, 1013], [1014, 1021], [1022, 1025], [1026, 1032], [1032, 1033], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0]]\n",
      "Answers: {'text': ['1851–1921'], 'answer_start': [353]}\n",
      "Question: What was the lifespan of John Augustine Zahm?\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋의 첫 번째 샘플을 확인\n",
    "print('Context length:', len(tokenized_dataset['train']['context'][0]))\n",
    "print('Input IDs length:', len(tokenized_dataset['train']['input_ids'][0]))\n",
    "print('Attention mask length:', len(tokenized_dataset['train']['attention_mask'][0]))\n",
    "print('Offset mapping length:', len(tokenized_dataset['train']['offset_mapping'][0]))\n",
    "print('Answers length:', len(tokenized_dataset['train']['answers'][0]))\n",
    "print('*'*50)\n",
    "print('Context:', tokenized_dataset['train']['context'][0])\n",
    "# input id : 토크나이저의 어휘 사전(vocabulary)에 등록된 토큰의 고유 번호\n",
    "print('Input IDs:', tokenized_dataset['train']['input_ids'][0])\n",
    "# attention mask : 어떤 토큰을 실제로 처리할지 여부를 나타냄, 1은 처리, 0은 무시 \n",
    "# 모델 입력 길이 맞춰야 해서 짧은 문장 뒤에 PAD토큰 채움->모델이 PAD 무시하도록 알려주는게 attention mask\n",
    "print('Attention mask:', tokenized_dataset['train']['attention_mask'][0])\n",
    "# offset mapping : 토크나이저가 자른 각 토큰이 원래 문장에서 어느 위치(시작, 끝)\n",
    "# tokenizer가 전처리한 후, 각 토큰이 원래 문장에서 어디에 위치하는지 알려줌\n",
    "print('Offset mapping:', tokenized_dataset['train']['offset_mapping'][0])\n",
    "print('Answers:', tokenized_dataset['train']['answers'][0])\n",
    "print('Question:', tokenized_dataset['train']['question'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee80712c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index Token        Offset          Matched Text\n",
      "--------------------------------------------------\n",
      "0     [CLS]        (0, 0)   ''\n",
      "1     in           (0, 2)   'In'\n",
      "2     january      (3, 10)   'January'\n",
      "3     2013         (11, 15)   '2013'\n",
      "4     ,            (15, 16)   ','\n",
      "5     destiny      (17, 24)   'Destiny'\n",
      "6     '            (24, 25)   '''\n",
      "7     s            (25, 26)   's'\n",
      "8     child        (27, 32)   'Child'\n",
      "9     released     (33, 41)   'released'\n",
      "10    love         (42, 46)   'Love'\n",
      "11    songs        (47, 52)   'Songs'\n",
      "12    [SEP]        (0, 0)   ''\n"
     ]
    }
   ],
   "source": [
    "context = \"In January 2013, Destiny's Child released Love Songs\"\n",
    "tokens = tokenizer.tokenize(context)\n",
    "encoding = tokenizer(context, return_offsets_mapping=True)\n",
    "tokens = tokenizer.convert_ids_to_tokens(encoding['input_ids'])\n",
    "offsets = encoding['offset_mapping']\n",
    "\n",
    "print(f\"{'Index':<5} {'Token':<12} {'Offset':<15} {'Matched Text'}\")\n",
    "print(\"-\" * 50)\n",
    "for i, (tok, (start, end)) in enumerate(zip(tokens, offsets)):\n",
    "    matched_text = context[start:end]\n",
    "    print(f\"{i:<5} {tok:<12} ({start}, {end})   '{matched_text}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0a3aa16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 80/80 [00:00<00:00, 3212.18 examples/s]\n",
      "Map: 100%|██████████| 20/20 [00:00<00:00, 2471.45 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# 전처리 함수 정의\n",
    "def preprocess_function(examples):\n",
    "    # 질문 텍스트에서 앞뒤 공백 제거\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "\n",
    "    # 토크나이즈 수행: 질문과 문맥을 함께 인코딩\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=384,               # 최대 길이 설정 (384 토큰)\n",
    "        truncation=\"only_second\",    # context가 너무 길 경우 context만 자르기\n",
    "        return_offsets_mapping=True, # 각 토큰이 원문에서 차지하는 문자 범위 반환\n",
    "        padding=\"max_length\",        # 길이 맞추기 (패딩 추가)\n",
    "    )\n",
    "\n",
    "    # offset_mapping은 나중에 사용하므로 따로 꺼내고, inputs에서는 제거\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    answers = examples[\"answers\"]    # 정답(annotation) 정보 가져오기\n",
    "\n",
    "    start_positions = []  # 정답의 시작 토큰 인덱스 리스트\n",
    "    end_positions = []    # 정답의 끝 토큰 인덱스 리스트\n",
    "\n",
    "    # 각 데이터 샘플에 대해 반복\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        answer = answers[i]\n",
    "        start_char = answer[\"answer_start\"][0]             # 정답 시작 문자 위치\n",
    "        end_char = start_char + len(answer[\"text\"][0])     # 정답 끝 문자 위치\n",
    "\n",
    "        # 각 토큰이 질문/문맥/패딩 중 어디에 속하는지 알려주는 리스트\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "        # context 영역의 시작 인덱스 찾기\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "\n",
    "        # context 영역의 끝 인덱스 찾기\n",
    "        while idx < len(sequence_ids) and sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        # 정답이 context 범위를 벗어난 경우 → 학습에서 무시할 값 (0, 0) 지정\n",
    "        if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            # context 내에서 정답 시작 토큰 찾기\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            # context 내에서 정답 끝 토큰 찾기\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "\n",
    "    # inputs에 정답 토큰 위치 정보 추가\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs\n",
    "\n",
    "# 전처리 함수 적용: 여러 샘플(batch)을 한 번에 처리하며, 원래의 컬럼은 제거\n",
    "tokenized_squad = squad.map(preprocess_function, batched=True, remove_columns=squad[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "071a5fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 모델 로드\n",
    "data_collator = DefaultDataCollator()\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"distilbert/distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b5f2381",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gayeon39/miniconda3/envs/da/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6/6 00:02, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>5.871279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>5.820331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>5.797934</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gayeon39/miniconda3/envs/da/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6, training_loss=5.8269602457682295, metrics={'train_runtime': 4.4481, 'train_samples_per_second': 53.956, 'train_steps_per_second': 1.349, 'total_flos': 23517558005760.0, 'train_loss': 5.8269602457682295, 'epoch': 3.0})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 학습\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../experiment/qa_bert\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=True,\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_squad[\"train\"],\n",
    "    eval_dataset=tokenized_squad[\"test\"],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23adc317",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Eiffel Tower.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "qa_pipeline = pipeline(\"question-answering\", model=trainer.model, tokenizer=tokenizer)\n",
    "example = {\n",
    "    \"context\": \"The capital of France is Paris. It is known for the Eiffel Tower.\",\n",
    "    \"question\": \"What is the capital of France?\"\n",
    "}\n",
    "result = qa_pipeline(example)\n",
    "print(f\"Answer: {result['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05f3d133",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Eiffel Tower.\n"
     ]
    }
   ],
   "source": [
    "# 학습한 모델 가져오기\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"../experiment/qa_bert\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "qa_pipeline = pipeline(\"question-answering\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "example = {\n",
    "    \"context\": \"The capital of France is Paris. It is known for the Eiffel Tower.\",\n",
    "    \"question\": \"What is the capital of France?\"\n",
    "}\n",
    "result = qa_pipeline(example)\n",
    "print(f\"Answer: {result['answer']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "da",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
