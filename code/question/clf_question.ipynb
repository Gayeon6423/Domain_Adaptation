{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "826e4034",
   "metadata": {},
   "source": [
    "- 데이터 파일을 읽어 질문과 ID를 추출\n",
    "- InferSent 모델 및 Google USE 임베딩 함수 준비\n",
    "- 분류기(MLP) 모델 로드\n",
    "- 질문을 배치 단위로 임베딩 생성 → 분류기 예측 → 질문 유형(q_type) 리스트 생성\n",
    "- 결과를 원본 데이터에 반영하여 새 파일로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c5e2a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current path:  /home/gayeon43/gayeon/[DA]/code/classify_question\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "from __future__ import absolute_import, division\n",
    "from tensorflow.python.framework.ops import enable_eager_execution\n",
    "enable_eager_execution()\n",
    "import io\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import json\n",
    "import torch\n",
    "import nltk\n",
    "import numpy as np\n",
    "import re\n",
    "from infersent import InferSent\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.logging.set_verbosity(0)\n",
    "import tensorflow_hub as hub\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# nltk.download('punkt_tab')\n",
    "\n",
    "# Set PATHs\n",
    "PATH_SENTEVAL = '../utils/SentEval'  # specify SentEval root if not installed\n",
    "PATH_TO_DATA = ''  # not necessary for inference\n",
    "MODEL_VERSION = 1\n",
    "# 300차원의 사전 단어 임베딩 벡터\n",
    "PATH_TO_W2V = '../../data/glove/glove.840B.300d.txt' if MODEL_VERSION == 1 else '../SentEval/fasttext/crawl-300d-2M.vec'\n",
    "# 사전 학습된 임베딩 모델 Infersent1\n",
    "MODEL_PATH = \"../../model/encoder/infersent%s.pkl\" % MODEL_VERSION\n",
    "\n",
    "## glove.txt: 각 단어를 300차원의 벡터로 표현하는 사전 임베딩 파일\n",
    "# ->infersent모델이 입력 문장 임베딩할 때, 각 단어 벡터로 변환에 사용\n",
    "## infersent1.pkl: 사전 학습된 문장 임베딩 모델의 가중치 파일\n",
    "# ->여러 단어 임베딩(GloVe)받아 문장 전체의 의미를 하나의 벡터로 변환\n",
    "# -> GloVe:단어를 벡터로 / InferSent:문장을 벡터로\n",
    "\n",
    "sys.path.insert(0, PATH_SENTEVAL)\n",
    "import senteval\n",
    "from senteval.tools.classifier import MLP\n",
    "\n",
    "# 경로(폴더 위치), 현재 작업 경로\n",
    "print('current path: ', os.getcwd())\n",
    "# 모듈 찾을 때 참조하는 경로 리스트, 상대 경로 해석(sys.path.insert로 수정)\n",
    "# sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "994dbc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentence(text: str) -> str:\n",
    "    # 소유격, 축약형 's 분리\n",
    "    text = re.sub(r\"(\\w+)'s\", r\"\\1 's\", text)\n",
    "    # 축약형 단순 변환s\n",
    "    text = text.replace(\"n't\", \" not\")\n",
    "    text = text.replace(\"'re\", \" are\")\n",
    "    text = text.replace(\"'ve\", \" have\")\n",
    "    text = text.replace(\"'ll\", \" will\")\n",
    "    text = text.replace(\"'d\", \" would\")\n",
    "    text = text.replace(\"'m\", \" am\")\n",
    "    text = text.replace(\"What's\", \"What is\") \n",
    "    # 하이픈 단어 분리\n",
    "    text = re.sub(r\"(\\w+)-(\\w+)\", r\"\\1 \\2\", text)\n",
    "    # 불필요한 특수문자 제거\n",
    "    text = re.sub(r\"[^A-Za-z0-9\\s\\?]\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "def preprocess_sentences(sent_list):\n",
    "    return [clean_sentence(s) for s in sent_list]\n",
    "\n",
    "def load_file(fpath):\n",
    "    \"\"\"\n",
    "    데이터 파일을 읽어 질문과 질문 ID를 추출\n",
    "    tgt2idx = {'ABBR': 0, 'DESC': 1, 'ENTY': 2,'HUM': 3, 'LOC': 4, 'NUM': 5}\n",
    "    \"\"\"\n",
    "    qa_data = []\n",
    "    qa_ids = []\n",
    "\n",
    "    if fpath.endswith('.jsonl'):\n",
    "    # JSONL 파일 처리(Ex.HotPot)\n",
    "        input_data = []\n",
    "        with io.open(fpath, 'r', encoding='utf-8') as f:\n",
    "            for example in f:\n",
    "                if \"header\" in json.loads(example):\n",
    "                    continue\n",
    "                paragraph = json.loads(example)\n",
    "                input_data.append(paragraph)\n",
    "        input_data = input_data ################ 개수 줄이기 ################\n",
    "        for paragraph in input_data:\n",
    "            for qa in paragraph['qas']:\n",
    "                qa_data.append(qa['question'].split())\n",
    "                qa_ids.append(qa['id'])\n",
    "        return qa_data, qa_ids     \n",
    "      \n",
    "    elif fpath.endswith('.json'):\n",
    "        # JSON 파일 처리(Ex.SQuAD, CNN)\n",
    "        with io.open(fpath, 'r', encoding='utf-8') as f:\n",
    "            input_data = json.load(f)[\"data\"]\n",
    "            input_data = input_data ################ 개수 줄이기 ################\n",
    "        for entry in input_data:\n",
    "            for paragraph in entry[\"paragraphs\"]:\n",
    "                for qa in paragraph[\"qas\"]:\n",
    "                    qa_data.append(qa['question'].split())\n",
    "                    qa_ids.append(qa['id'])\n",
    "        return qa_data, qa_ids\n",
    "\n",
    "def prepare(params, samples):\n",
    "    \"\"\"\n",
    "    InferSent 모델에 사용할 단어 사전을 구축하는 함수\n",
    "    축약형·소유격·하이픈 결합 단어 전처리\n",
    "    \"\"\"\n",
    "    sentences = [' '.join(sent) if sent != [] else '.' for sent in samples]\n",
    "    sentences = preprocess_sentences(sentences)\n",
    "    params['infersent'].build_vocab(sentences, tokenize=False) # 모델 불러옴\n",
    "\n",
    "def batcher(params, batch):\n",
    "    \"\"\"\n",
    "    입력된 질문 배치를 임베딩 벡터로 변환하는 함수\n",
    "    \"\"\"\n",
    "    # 1. 각 질문(토큰 리스트)을 문자열로 합침\n",
    "    sentences = [' '.join(sent) if sent != [] else '.' for sent in batch]\n",
    "    # 2. 축약형·소유격·하이픈 결합 단어 전처리\n",
    "    sentences = preprocess_sentences(sentences)\n",
    "    # 3. InferSent 모델로 임베딩 생성\n",
    "    embeddings1 = params['infersent'].encode(sentences, bsize=params['classifier']['batch_size'], tokenize=False)\n",
    "    # 4. Google Universal Sentence Encoder로 임베딩 생성\n",
    "    embeddings2 = params['google_use'](sentences)\n",
    "    embeddings2 = embeddings2.numpy()\n",
    "    # 5. 두 임베딩을 합쳐서 반환 (문장 의미를 더 풍부하게 표현)\n",
    "    return np.concatenate((embeddings1, embeddings2), axis=-1)\n",
    "\n",
    "def make_embed_fn(module):\n",
    "    embed = hub.load(module)\n",
    "    f = embed.signatures[\"default\"]\n",
    "    return lambda x: f(tf.constant(x))[\"default\"]\n",
    "\n",
    "def getEmbeddings(qa_data, params):\n",
    "    # 전체 질문 리스트를 배치 단위로 나눠 임베딩을 생성하는 함수\n",
    "    out_embeds = []\n",
    "    # 배치 크기만큼 반복하며 임베딩 생성\n",
    "    for ii in range(0, len(qa_data), params['classifier']['batch_size']):\n",
    "        batch = qa_data[ii:ii + params['classifier']['batch_size']]\n",
    "        # batcher 함수로 임베딩 생성\n",
    "        embeddings = batcher(params, batch)\n",
    "        out_embeds.append(embeddings)\n",
    "    # 모든 배치 임베딩을 하나로 합쳐 반환(행렬 세로 방향으로 이어붙임)\n",
    "    # 입력: [batch_size개의 문장] -> 출력: (batch_size, emb_dim)\n",
    "    return np.vstack(out_embeds) # (N, 4096+512)\n",
    "\n",
    "def update_squad(fpath, q_type, q_type_prob,q_ids):\n",
    "    with io.open(fpath, 'r', encoding='utf-8') as f:\n",
    "        input_data = json.load(f)[\"data\"]\n",
    "        input_data = input_data ################ 개수 줄이기 ################\n",
    "    # 각 질문에 대해 분류 결과를 추가\n",
    "    total_idx = 0\n",
    "    for entry in input_data:\n",
    "        for paragraph in entry[\"paragraphs\"]:\n",
    "            for qa in paragraph['qas']:\n",
    "                # 예측된 질문 유형과 유형 확률값을 해당 질문에 추가\n",
    "                if qa['id'] == q_ids[total_idx]:\n",
    "                    qa['q_type'] = q_type[total_idx]\n",
    "                    qa['q_type_prob'] = q_type_prob[total_idx]\n",
    "                    total_idx += 1\n",
    "                else:\n",
    "                    # 질문 ID가 맞지 않으면 경고 출력\n",
    "                    print('Can not match qid:', q_ids[total_idx])\n",
    "                    \n",
    "def convert_squad_format(data):\n",
    "    '''\n",
    "    hotpot format -> squad format\n",
    "    '''\n",
    "    converted_data = []\n",
    "    for item in data:\n",
    "        title = item.get('id', '') \n",
    "        context = item.get('context', '')\n",
    "        qas = item.get('qas', [])\n",
    "\n",
    "        new_qas = []\n",
    "        for qa in qas:\n",
    "            new_qa = {\n",
    "                'id': qa.get('id', ''),\n",
    "                'question': qa.get('question', ''),\n",
    "                'answers': qa.get('answers', []),\n",
    "                'q_type': qa.get('q_type', None),\n",
    "                'q_type_prob': qa.get('q_type_prob', None)\n",
    "            }\n",
    "            new_qas.append(new_qa)\n",
    "\n",
    "        new_entry = {\n",
    "            'title': title,\n",
    "            'paragraphs': [{'context': context,'qas': new_qas}]\n",
    "}\n",
    "        converted_data.append(new_entry)\n",
    "    return converted_data\n",
    "\n",
    "def update_file(fpath, q_type, q_type_prob, q_ids):\n",
    "    \"\"\"\n",
    "    기존 파일에 질문 유형(q_type)과 질문 유형 확률(q_type_prob)을 추가하는 함수\n",
    "    * format은 squad 형식으로 통합 *\n",
    "    \"\"\"\n",
    "    if fpath.endswith('.jsonl'):\n",
    "        # JSONL 파일 처리(Ex.HotPot)\n",
    "        input_data = []\n",
    "        with io.open(fpath, 'r', encoding='utf-8') as f:\n",
    "            for example in f:\n",
    "                if \"header\" in json.loads(example): \n",
    "                    # header가 포함된 줄은 건너뜀\n",
    "                    continue\n",
    "                paragraph = json.loads(example)\n",
    "                input_data.append(paragraph)\n",
    "        input_data = input_data ################ 개수 줄이기 ################\n",
    "        total_idx = 0\n",
    "\n",
    "        for paragraph in input_data:\n",
    "            for qa in paragraph['qas']:\n",
    "                # 예측된 질문 유형과 유형 확률값을 해당 질문에 추가\n",
    "                if qa['id'] == q_ids[total_idx]:\n",
    "                    qa['q_type'] = q_type[total_idx]\n",
    "                    qa['q_type_prob'] = q_type_prob[total_idx]\n",
    "                    total_idx += 1\n",
    "                else:\n",
    "                    # 질문 ID가 맞지 않으면 경고 출력\n",
    "                    print('Can not match id:', q_ids[total_idx])\n",
    "                    \n",
    "                    \n",
    "        # format을 squad 형식으로 변환\n",
    "        input_data = convert_squad_format(input_data)\n",
    "        \n",
    "    elif fpath.endswith('.json'):\n",
    "        # JSON 파일 처리(Ex.SQuAD, CNN)\n",
    "        with io.open(fpath, 'r', encoding='utf-8') as f:\n",
    "            input_data = json.load(f)[\"data\"]\n",
    "            input_data = input_data ################ 개수 줄이기 ################\n",
    "        # 각 질문에 대해 분류 결과를 추가\n",
    "        total_idx = 0\n",
    "        for entry in input_data:\n",
    "            for paragraph in entry[\"paragraphs\"]:\n",
    "                for qa in paragraph['qas']:\n",
    "                    # 예측된 질문 유형과 유형 확률값을 해당 질문에 추가\n",
    "                    if qa['id'] == q_ids[total_idx]:\n",
    "                        qa['q_type'] = q_type[total_idx]\n",
    "                        qa['q_type_prob'] = q_type_prob[total_idx]\n",
    "                        total_idx += 1\n",
    "                    else:\n",
    "                        # 질문 ID가 맞지 않으면 경고 출력\n",
    "                        print('Can not match qid:', q_ids[total_idx])\n",
    "        \n",
    "    # 새로운 파일로 저장 (원본 파일명.jsonl->_qtype_prob.jsonl)\n",
    "    if fpath.endswith('.jsonl'):\n",
    "        with open(fpath[:-6]+'_qtype_prob.jsonl', 'w') as f:\n",
    "            for sample in input_data:\n",
    "                f.write(json.dumps(sample)+'\\n')\n",
    "        f.close()\n",
    "    elif fpath.endswith('.json'):\n",
    "        with open(fpath[:-5]+'_qtype_prob.jsonl', 'w') as f:\n",
    "            for sample in input_data:\n",
    "                f.write(json.dumps(sample)+'\\n')\n",
    "        f.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636cbde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set\n",
    "encoder = make_embed_fn(\"https://tfhub.dev/google/universal-sentence-encoder-large/3\")\n",
    "params_senteval = {'task_path': PATH_TO_DATA, 'usepytorch': True, 'kfold': 5}\n",
    "params_senteval['classifier'] = {'nhid': 512, 'optim': 'rmsprop', 'batch_size': 16,\n",
    "                                 'tenacity': 5, 'epoch_size': 4}\n",
    "params_senteval['google_use'] = encoder\n",
    "params_model = {'bsize': 16, 'word_emb_dim': 300, 'enc_lstm_dim': 2048,\n",
    "                'pool_type': 'max', 'dpout_model': 0.0, 'version': MODEL_VERSION}\n",
    "model = InferSent(params_model)\n",
    "model.load_state_dict(torch.load(MODEL_PATH))\n",
    "model.set_w2v_path(PATH_TO_W2V)\n",
    "params_senteval['infersent'] = model.cuda().eval()\n",
    "\n",
    "# Set Parameter\n",
    "q_type = []\n",
    "q_type_prob = []\n",
    "clf = MLP(params_senteval['classifier'], inputdim=4096+512, nclasses=6, batch_size=16)\n",
    "clf.model.load_state_dict(torch.load('../../model/clf_trec/qc4qa_model.pth'))  \n",
    "clf.model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a024a2a",
   "metadata": {},
   "source": [
    "- Classification + Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9845973a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['To', 'whom', 'did', 'the', 'Virgin', 'Mary', 'allegedly', 'appear', 'in', '1858', 'in', 'Lourdes', 'France?']\n",
      "5733be284776f41900661182\n",
      "87599\n"
     ]
    }
   ],
   "source": [
    "# Source : SQuAD\n",
    "# Target : HotPot, CNN\n",
    "squad_file_path = '../../data/squad/train-v1.1.json'\n",
    "hotpot_file_path = '../../data/hotpot/hotpotqa_train_classified.jsonl'\n",
    "cnn_file_path = '../../data/cnn/cnn_train_classified.json'\n",
    "\n",
    "all_qs, all_q_ids = load_file(squad_file_path)\n",
    "print(all_qs[0])\n",
    "print(all_q_ids[0])\n",
    "print(len(all_q_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f002046f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=4608, out_features=512, bias=True)\n",
       "  (1): Dropout(p=0.0, inplace=False)\n",
       "  (2): Sigmoid()\n",
       "  (3): Linear(in_features=512, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set\n",
    "encoder = make_embed_fn(\"https://tfhub.dev/google/universal-sentence-encoder-large/3\")\n",
    "params_senteval = {'task_path': PATH_TO_DATA, 'usepytorch': True, 'kfold': 5}\n",
    "params_senteval['classifier'] = {'nhid': 512, 'optim': 'rmsprop', 'batch_size': 16,\n",
    "                                 'tenacity': 5, 'epoch_size': 4}\n",
    "params_senteval['google_use'] = encoder\n",
    "params_model = {'bsize': 16, 'word_emb_dim': 300, 'enc_lstm_dim': 2048,\n",
    "                'pool_type': 'max', 'dpout_model': 0.0, 'version': MODEL_VERSION}\n",
    "model = InferSent(params_model)\n",
    "model.load_state_dict(torch.load(MODEL_PATH))\n",
    "model.set_w2v_path(PATH_TO_W2V)\n",
    "params_senteval['infersent'] = model.cuda().eval()\n",
    "\n",
    "# Set Parameter\n",
    "clf = MLP(params_senteval['classifier'], inputdim=4096+512, nclasses=6, batch_size=16)\n",
    "clf.model.load_state_dict(torch.load('../../model/clf_trec/qc4qa_model.pth'))  \n",
    "clf.model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d2d664",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_type = []\n",
    "q_type_prob = []\n",
    "with torch.no_grad(): \n",
    "    # 질문을 배치 단위로 임베딩->MLP예측->결과 누적\n",
    "    for i in range(0, len(all_qs), 1000): \n",
    "        qs = all_qs[i:1000+i] # 현재 배치에 해당하는 질문 1000개 선택\n",
    "        prepare(params_senteval, qs) # InferSent모델에 맞게 단어 사전 구축\n",
    "        # 선택된 질문을 InferSent와 Google USE로 임베딩 벡터로 변환\n",
    "        embeds = getEmbeddings(qs, params_senteval) \n",
    "        # 임베딩된 질문을 MLP 분류기에 입력하여 질문 유형 예측\n",
    "        out = clf.predict(embeds)\n",
    "        out_proba = clf.predict_proba(embeds)\n",
    "        out_proba = np.round(out_proba, 3)\n",
    "        # 예측 결과를 리스트로 변환하여 전체 결과(q_type)에 추가\n",
    "        q_type += np.array(out).squeeze().astype(int).tolist()\n",
    "        q_type_prob += out_proba.tolist()\n",
    "        \n",
    "# SQuAD update\n",
    "update_file(squad_file_path, q_type, q_type_prob, all_q_ids)\n",
    "# Hotpot update\n",
    "# update_file(hotpot_file_path, q_type, q_type_prob, all_q_ids)\n",
    "# CNN update\n",
    "# update_file(cnn_file_path, q_type, q_type_prob, all_q_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "987cd05c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['title', 'paragraphs'])\n",
      "dict_keys(['title', 'paragraphs'])\n",
      "dict_keys(['title', 'paragraphs'])\n"
     ]
    }
   ],
   "source": [
    "squad = '../../data/squad/train-v1.1_qtype_prob.jsonl'\n",
    "cnn = '../../data/cnn/cnn_train_classified_qtype_prob.jsonl'\n",
    "hotpot = '../../data/hotpot/hotpotqa_train_classified_qtype_prob.jsonl'\n",
    "\n",
    "squads = []\n",
    "with io.open(squad, 'r', encoding='utf-8') as f:\n",
    "    for example in f:\n",
    "        paragraph = json.loads(example)\n",
    "        squads.append(paragraph)\n",
    "cnns = []\n",
    "with io.open(cnn, 'r', encoding='utf-8') as f:\n",
    "    for example in f:\n",
    "        paragraph = json.loads(example)\n",
    "        cnns.append(paragraph)        \n",
    "hotpots = []\n",
    "with io.open(hotpot, 'r', encoding='utf-8') as f:\n",
    "    for example in f:\n",
    "        paragraph = json.loads(example)\n",
    "        hotpots.append(paragraph)\n",
    "\n",
    "print(squads[0].keys())\n",
    "print(cnns[0].keys())\n",
    "print(hotpots[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b34cc0ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "442\n",
      "dict_keys(['context', 'qas'])\n",
      "{'answers': [{'answer_start': 515, 'text': 'Saint Bernadette Soubirous'}], 'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?', 'id': '5733be284776f41900661182', 'q_type': 3, 'q_type_prob': [0.0, 0.0, 0.016, 0.983, 0.001, 0.0]}\n",
      "dict_keys(['answers', 'question', 'id', 'q_type', 'q_type_prob'])\n",
      "**************************************************\n",
      "93627\n",
      "dict_keys(['context', 'qas'])\n",
      "{'answers': [{'answer_start': 2070, 'text': 'Globe'}], 'question': 'on the Democratic side , the Register backs Clinton , while the @placeholder picks Obama', 'id': 'training/6dc32db9379e43971ad93007e76c5347e213f21a', 'q_type': 3, 'q_type_prob': [0.001, 0.001, 0.171, 0.76, 0.064, 0.002]}\n",
      "dict_keys(['answers', 'question', 'id', 'q_type', 'q_type_prob'])\n",
      "**************************************************\n",
      "72928\n",
      "dict_keys(['context', 'qas'])\n",
      "{'id': '5a879ab05542996e4f30887e#0', 'question': 'The Oberoi family is part of a hotel company that has a head office in what city?', 'answers': ['Delhi'], 'q_type': 3, 'q_type_prob': [0.0, 0.0, 0.002, 0.562, 0.435, 0.001]}\n",
      "dict_keys(['id', 'question', 'answers', 'q_type', 'q_type_prob'])\n"
     ]
    }
   ],
   "source": [
    "print(len(squads))\n",
    "print(squads[0]['paragraphs'][0].keys())\n",
    "print(squads[0]['paragraphs'][0]['qas'][0])\n",
    "print(squads[0]['paragraphs'][0]['qas'][0].keys())\n",
    "print('*'*50)\n",
    "print(len(cnns))\n",
    "print(cnns[0]['paragraphs'][0].keys())\n",
    "print(cnns[0]['paragraphs'][0]['qas'][0])\n",
    "print(cnns[0]['paragraphs'][0]['qas'][0].keys())\n",
    "print('*'*50)\n",
    "print(len(hotpots))\n",
    "print(hotpots[0]['paragraphs'][0].keys())\n",
    "print(hotpots[0]['paragraphs'][0]['qas'][0])\n",
    "print(hotpots[0]['paragraphs'][0]['qas'][0].keys())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "da",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
