{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9bdcd57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gayeon39/miniconda3/envs/da/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-09-25 17:10:33.342660: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"NCCL_P2P_DISABLE\"] = \"3\"\n",
    "os.environ[\"NCCL_IB_DISABLE\"] = \"3\"\n",
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
    "from datasets import load_dataset, Dataset\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "### Config\n",
    "BASE_MODEL = \"google/flan-t5-base\"\n",
    "GENERATE_QUESTION_MODEL = \"../../experiment/generate_data/generate_question_flan_t5\"\n",
    "TRAIN_SQUAD_PATH = '../../data/squad/train-v1.1_classified.json'\n",
    "DEV_SQUAD_PATH = '../../data/squad/dev-v1.1.json'\n",
    "TRAIN_CNN_PATH = '../../data/cnn/cnn_train_classified.json'\n",
    "DEV_CNN_PATH = '../../data/cnn/cnn_dev.json'\n",
    "\n",
    "with open(TRAIN_SQUAD_PATH, 'r') as f:\n",
    "    train_squad_data = json.load(f)\n",
    "with open(DEV_SQUAD_PATH, 'r') as f:\n",
    "    dev_squad_data = json.load(f)\n",
    "with open(TRAIN_CNN_PATH, 'r') as f:\n",
    "    train_cnn_data = json.load(f)\n",
    "with open(DEV_CNN_PATH, 'r') as f:\n",
    "    dev_cnn_data = json.load(f)\n",
    "    \n",
    "target_context = []\n",
    "for article in train_cnn_data['data']:\n",
    "    for para in article['paragraphs']:\n",
    "        target_context.append(para['context'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "036df5b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "/home/gayeon39/miniconda3/envs/da/lib/python3.12/site-packages/accelerate/utils/modeling.py:1614: UserWarning: The following device_map keys do not match any submodules in the model: ['decoder.embed_tokens', 'encoder.embed_tokens']\n",
      "  warnings.warn(\n",
      "We've detected an older driver with an RTX 4000 series GPU. These drivers have issues with P2P. This can affect the multi-gpu inference when using accelerate device_map.Please make sure to update your driver to the latest version which resolves this.\n"
     ]
    }
   ],
   "source": [
    "# Load base model \n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(BASE_MODEL, device_map=\"auto\", torch_dtype=torch.float16)\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\"\n",
    ")\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3923257e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the dataset\n",
    "def create_question_prompt(input_text, question_type):\n",
    "  # Define the prompt format for each question type\n",
    "  prompt = f\"\"\"\n",
    "  Given the context, generate a question based on the specified question type ({question_type}).\n",
    "\n",
    "  Question Type: {question_type}\n",
    "  Example: \n",
    "    Input: \"{input_text}\"\n",
    "    Output: [generated_question]\n",
    "  \"\"\"\n",
    "  return prompt.strip()\n",
    "\n",
    "\n",
    "# Step 3: Tokenize the data\n",
    "# def tokenize_gen_question(example,tokenizer, max_input_length=512, max_target_length=128):\n",
    "#   prompt = create_question_prompt(example[\"context\"], example[\"q_type\"])\n",
    "#   # Input 토크나이징 (context + question_type)\n",
    "#   inputs = tokenizer(prompt,padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "#   # Traget 토크나이징 (question)\n",
    "#   targets = tokenizer(example[\"question\"], padding=\"max_length\", truncation=True, max_length=64, return_tensors=\"pt\")\n",
    "#   return {\"input_ids\": inputs[\"input_ids\"].squeeze(), \"attention_mask\": inputs[\"attention_mask\"].squeeze(), \"labels\": targets[\"input_ids\"].squeeze()}\n",
    "\n",
    "def tokenize_gen_question(example, tokenizer, max_input_length=512, max_target_length=128):\n",
    "    prompt = create_question_prompt(example[\"context\"], example[\"q_type\"])\n",
    "    \n",
    "    # 입력 토큰화 (context + question_type)\n",
    "    model_inputs = tokenizer(\n",
    "        prompt,\n",
    "        max_length=max_input_length,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    # 타겟 토큰화 (question)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            example[\"question\"],\n",
    "            max_length=max_target_length,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\"\n",
    "        )\n",
    "\n",
    "    # padding 토큰(-100으로 변환 → loss 계산에서 무시)\n",
    "    labels[\"input_ids\"] = [\n",
    "        (l if l != tokenizer.pad_token_id else -100) for l in labels[\"input_ids\"]\n",
    "    ]\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0a3ad14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/269 [00:00<?, ? examples/s]/home/gayeon39/miniconda3/envs/da/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:4007: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 269/269 [00:00<00:00, 1005.60 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(qa_data):  269\n",
      "qa_data[0]: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.',\n",
       " 'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?',\n",
       " 'answer': 'Saint Bernadette Soubirous',\n",
       " 'q_type': 3}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_data = []\n",
    "\n",
    "for idx1 in range(0,len(train_squad_data['data'][:1])):\n",
    "    for idx2 in range(0,len(train_squad_data['data'][idx1]['paragraphs'])):\n",
    "        qas = train_squad_data['data'][idx1]['paragraphs'][idx2]['qas']\n",
    "        question_li = [qas[i]['question'] for i in range(len(qas))]\n",
    "        q_type_li = [qas[i]['q_type'] for i in range(len(qas))]\n",
    "        answer_li = [qas[i]['answers'][0]['text'] for i in range(len(qas))]\n",
    "        context_li = [train_squad_data['data'][idx1]['paragraphs'][idx2]['context']] * (len(qas))\n",
    "\n",
    "        for i,j,x,z in zip(context_li, question_li, answer_li, q_type_li):    \n",
    "            qa_data.append({\"context\":i, \"question\":j,\"answer\":x,\"q_type\":z})\n",
    "\n",
    "# Convert data into a Dataset object\n",
    "dataset = Dataset.from_pandas(pd.DataFrame(qa_data))\n",
    "# Apply the tokenizer to the dataset\n",
    "tokenized_dataset = dataset.map(lambda x: tokenize_gen_question(x, tokenizer), batched=False)\n",
    "\n",
    "print('len(qa_data): ', len(qa_data))\n",
    "print('qa_data[0]: ', end='')\n",
    "qa_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6fd9cf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2929649/1888134585.py:16: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='204' max='204' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [204/204 01:01, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define training arguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "output_dir=GENERATE_QUESTION_MODEL, # 최종 모델 저장 경로\n",
    "    per_device_train_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=5e-4,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"no\",  # Disable automatic saving\n",
    "    save_total_limit=2,\n",
    "    fp16=True,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# Train the model using the Trainer API\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    ")\n",
    "\n",
    "# Start the training\n",
    "trainer.train()\n",
    "trainer.save_model(GENERATE_QUESTION_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93919dd",
   "metadata": {},
   "source": [
    "- Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d387f177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the fine-tuned model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(GENERATE_QUESTION_MODEL)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(GENERATE_QUESTION_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba27d88f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Question: What was Albert Einstein's occupation?\n"
     ]
    }
   ],
   "source": [
    "# Define a function to generate questions based on the context and question type\n",
    "def generate_question(context, question_type):\n",
    "    # Create the prompt for the T5 model\n",
    "    prompt = f\"\"\"\n",
    "    Given the context, generate a question based on the specified question type ({question_type}).\n",
    "\n",
    "    Question Type: {question_type}\n",
    "    Example: \n",
    "      Input: \"{context}\"\n",
    "      Output: [generated_question]\n",
    "    \"\"\"\n",
    "\n",
    "    # Tokenize the prompt\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "    # Generate the question using the model\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"], \n",
    "        attention_mask=inputs[\"attention_mask\"], \n",
    "        max_length=64,  # Limit the length of the generated question\n",
    "        num_beams=4,    # Use beam search for better quality\n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "    # Decode the generated question\n",
    "    generated_question = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated_question\n",
    "\n",
    "# Test with an example context\n",
    "context = \"Albert Einstein was a theoretical physicist who developed the theory of relativity.\"\n",
    "question_type = \"ENTY\"  # Choose the question type (ABBR, DESC, ENTY, HUM, LOC, NUM)\n",
    "\n",
    "# Generate the question\n",
    "generated_question = generate_question(context, question_type)\n",
    "\n",
    "# Print the generated question\n",
    "print(f\"Generated Question: {generated_question}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ebd2d533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question Type: DESC\n",
      "Context: Shakespeare was an English playwright and poet. He is widely regarded as one of the greatest writers in the English language.\n",
      "Generated Question: What is the full name of the person who wrote Shakespeare?\n",
      "--------------------------------------------------\n",
      "Context: The Amazon River is the largest river in the world by discharge of water. It flows through South America, primarily in Brazil.\n",
      "Generated Question: What is the largest river in the world by discharge of water?\n",
      "--------------------------------------------------\n",
      "Question Type: ENTY\n",
      "Context: Shakespeare was an English playwright and poet. He is widely regarded as one of the greatest writers in the English language.\n",
      "Generated Question: What was Shakespeare's occupation?\n",
      "--------------------------------------------------\n",
      "Context: The Amazon River is the largest river in the world by discharge of water. It flows through South America, primarily in Brazil.\n",
      "Generated Question: What is the largest river in the world by discharge of water?\n",
      "--------------------------------------------------\n",
      "Question Type: ABBR\n",
      "Context: Shakespeare was an English playwright and poet. He is widely regarded as one of the greatest writers in the English language.\n",
      "Generated Question: What is the name of Shakespeare's most famous play?\n",
      "--------------------------------------------------\n",
      "Context: The Amazon River is the largest river in the world by discharge of water. It flows through South America, primarily in Brazil.\n",
      "Generated Question: What is the largest river in the world by discharge of water?\n",
      "--------------------------------------------------\n",
      "Question Type: HUM\n",
      "Context: Shakespeare was an English playwright and poet. He is widely regarded as one of the greatest writers in the English language.\n",
      "Generated Question: What is the name of Shakespeare's most famous play?\n",
      "--------------------------------------------------\n",
      "Context: The Amazon River is the largest river in the world by discharge of water. It flows through South America, primarily in Brazil.\n",
      "Generated Question: What is the largest river in the world by discharge of water?\n",
      "--------------------------------------------------\n",
      "Question Type: LOC\n",
      "Context: Shakespeare was an English playwright and poet. He is widely regarded as one of the greatest writers in the English language.\n",
      "Generated Question: What is the name of Shakespeare's most famous play?\n",
      "--------------------------------------------------\n",
      "Context: The Amazon River is the largest river in the world by discharge of water. It flows through South America, primarily in Brazil.\n",
      "Generated Question: What is the largest river in the world by discharge of water?\n",
      "--------------------------------------------------\n",
      "Question Type: NUM\n",
      "Context: Shakespeare was an English playwright and poet. He is widely regarded as one of the greatest writers in the English language.\n",
      "Generated Question: What is the name of Shakespeare's most famous play?\n",
      "--------------------------------------------------\n",
      "Context: The Amazon River is the largest river in the world by discharge of water. It flows through South America, primarily in Brazil.\n",
      "Generated Question: What is the largest river in the world by discharge of water?\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "question_type_list = ['DESC', 'ENTY', 'ABBR', 'HUM', 'LOC', 'NUM']\n",
    "context_list = [\n",
    "    \"Shakespeare was an English playwright and poet. He is widely regarded as one of the greatest writers in the English language.\",\n",
    "    \"The Amazon River is the largest river in the world by discharge of water. It flows through South America, primarily in Brazil.\"]\n",
    "\n",
    "for i in question_type_list:\n",
    "    print(f\"Question Type: {i}\")\n",
    "    for j in context_list:\n",
    "        print(f\"Context: {j}\")\n",
    "        print(f\"Generated Question: {generate_question(j, i)}\")\n",
    "        print('-'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a5e7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data for fine-tuning (you can replace this with your own dataset)\n",
    "data = [\n",
    "    {\n",
    "        \"context\": \"NASA is the United States government agency responsible for the civilian space program.\",\n",
    "        \"q_type\": \"ABBR\",\n",
    "        \"question\": \"What does the abbreviation 'NASA' stand for?\"\n",
    "    },\n",
    "    {\n",
    "        \"context\": \"Photosynthesis is the process by which green plants use sunlight to synthesize foods from carbon dioxide and water.\",\n",
    "        \"q_type\": \"DESC\",\n",
    "        \"question\": \"Can you describe the process of photosynthesis?\"\n",
    "    },\n",
    "    {\n",
    "        \"context\": \"Albert Einstein was a theoretical physicist who developed the theory of relativity.\",\n",
    "        \"q_type\": \"ENTY\",\n",
    "        \"question\": \"Who was Albert Einstein?\"\n",
    "    },\n",
    "    {\n",
    "        \"context\": \"The first president of the United States was George Washington.\",\n",
    "        \"q_type\": \"HUM\",\n",
    "        \"question\": \"Who was the first president of the United States?\"\n",
    "    },\n",
    "    {\n",
    "        \"context\": \"The Eiffel Tower is located in Paris, France.\",\n",
    "        \"q_type\": \"LOC\",\n",
    "        \"question\": \"Where is the Eiffel Tower located?\"\n",
    "    },\n",
    "    {\n",
    "        \"context\": \"There are seven continents on Earth.\",\n",
    "        \"q_type\": \"NUM\",\n",
    "        \"question\": \"How many continents are there on Earth?\"\n",
    "    }\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "da",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
