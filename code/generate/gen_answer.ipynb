{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177b5b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gayeon39/miniconda3/envs/da/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-10-01 13:37:48.888321: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"NCCL_P2P_DISABLE\"] = \"2\"\n",
    "os.environ[\"NCCL_IB_DISABLE\"] = \"2\"\n",
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
    "from datasets import load_dataset, Dataset\n",
    "from peft import get_peft_model, LoraConfig, TaskType, PeftModel\n",
    "import torch\n",
    "import pandas as pd\n",
    "import io\n",
    "\n",
    "### Config\n",
    "BASE_MODEL = \"google/flan-t5-base\"\n",
    "GENERATE_ANSWER_MODEL = \"../../experiment/generate_data/generate_answer_flan_t5_all_squad\"\n",
    "TRAIN_SQUAD_PATH = '../../data/squad/train-v1.1.json'\n",
    "DEV_SQUAD_PATH = '../../data/squad/dev-v1.1.json'\n",
    "TRAIN_HOTPOT_PATH = '../../data/target/hotpotqa_train_classified.jsonl'\n",
    "DEV_HOTPOT_PATH = '../../data/target/hotpotqa_dev.jsonl'\n",
    "\n",
    "with open(TRAIN_SQUAD_PATH, 'r') as f:\n",
    "    train_squad_data = json.load(f)\n",
    "with open(DEV_SQUAD_PATH, 'r') as f:\n",
    "    dev_squad_data = json.load(f)\n",
    "# with open(TRAIN_CNN_PATH, 'r') as f:\n",
    "#     train_cnn_data = json.load(f)\n",
    "# with open(DEV_CNN_PATH, 'r') as f:\n",
    "#     dev_cnn_data = json.load(f)\n",
    "TRAIN_HOTPOT = []\n",
    "with io.open(TRAIN_HOTPOT_PATH, 'r', encoding='utf-8') as f:\n",
    "    for sample in f:\n",
    "        TRAIN_HOTPOT.append(json.loads(sample))\n",
    "    \n",
    "target_context = []\n",
    "# for article in train_cnn_data['data']:\n",
    "#     for para in article['paragraphs']:\n",
    "#         target_context.append(para['context'])\n",
    "for article in TRAIN_HOTPOT:\n",
    "    target_context.append(article['context'])\n",
    "        \n",
    "target_question = []\n",
    "# for article in train_cnn_data['data']:\n",
    "#     for para in article['paragraphs']:\n",
    "#         for qa in para['qas']:\n",
    "#             target_question.append(qa['question'])\n",
    "for article in TRAIN_HOTPOT:\n",
    "    target_question.append(article['qas'][0]['question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df5b8556",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "/home/gayeon39/miniconda3/envs/da/lib/python3.12/site-packages/accelerate/utils/modeling.py:1614: UserWarning: The following device_map keys do not match any submodules in the model: ['decoder.embed_tokens', 'encoder.embed_tokens']\n",
      "  warnings.warn(\n",
      "We've detected an older driver with an RTX 4000 series GPU. These drivers have issues with P2P. This can affect the multi-gpu inference when using accelerate device_map.Please make sure to update your driver to the latest version which resolves this.\n"
     ]
    }
   ],
   "source": [
    "# Load base model \n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(BASE_MODEL, device_map=\"auto\", torch_dtype=torch.float16)\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\"\n",
    ")\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec5e98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_answer_prompt(context, question):\n",
    "    \"\"\"\n",
    "    Context와 Question이 주어졌을 때 Answer 추출을 위한 프롬프트 생성\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"Given the context and question, extract the answer from the context that best answers the question.\n",
    "    Context: \"{context}\"\n",
    "    Question: \"{question}\"\n",
    "    Answer: [extracted_answer]\"\"\"\n",
    "    return prompt.strip()\n",
    "\n",
    "def tokenize_gen_answer(example, tokenizer, max_input_length=512, max_target_length=128):\n",
    "    prompt = create_answer_prompt(example[\"context\"], example[\"question\"])\n",
    "    \"\"\"\n",
    "    QA 데이터를 토크나이징하는 함수\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_input_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    targets = tokenizer(\n",
    "        example[\"answer\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_target_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    labels = targets[\"input_ids\"].squeeze()\n",
    "    labels[labels == tokenizer.pad_token_id] = -100  # 중요!\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": inputs[\"input_ids\"].squeeze(), # 모델이 인코더에 넣을 토큰 ID들(독립변수)\n",
    "        \"attention_mask\": inputs[\"attention_mask\"].squeeze(),# 패딩 부분 무시하도록 표시\n",
    "        \"labels\": labels # 모델이 디코더에서 예측해야 할 정답 토큰 ID들(종속변수)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "17556b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1483/1483 [00:01<00:00, 1314.74 examples/s]\n",
      "Map: 100%|██████████| 1057/1057 [00:00<00:00, 1268.64 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train_qa_data = []\n",
    "\n",
    "for idx1 in range(0,len(train_squad_data['data'][:5])): ### 자르기\n",
    "    for idx2 in range(0,len(train_squad_data['data'][idx1]['paragraphs'])):\n",
    "        qas = train_squad_data['data'][idx1]['paragraphs'][idx2]['qas']\n",
    "        question_li = [qas[i]['question'] for i in range(len(qas))]\n",
    "        answer_li = [qas[i]['answers'][0]['text'] for i in range(len(qas))]\n",
    "        context_li = [train_squad_data['data'][idx1]['paragraphs'][idx2]['context']] * (len(qas))\n",
    "\n",
    "        for i,j,x in zip(context_li, question_li, answer_li):    \n",
    "            train_qa_data.append({\"context\":i, \"question\":j, \"answer\":x})\n",
    "\n",
    "# 데이터셋 생성 및 토크나이징 적용\n",
    "train_dataset = Dataset.from_pandas(pd.DataFrame(train_qa_data))\n",
    "train_dataset = train_dataset.map(lambda x: tokenize_gen_answer(x, tokenizer), batched=False)\n",
    "\n",
    "dev_qa_data = []\n",
    "for idx1 in range(len(dev_squad_data['data'][:2])):   # dev 전체 사용\n",
    "    for idx2 in range(len(dev_squad_data['data'][idx1]['paragraphs'])):\n",
    "        qas = dev_squad_data['data'][idx1]['paragraphs'][idx2]['qas']\n",
    "        question_li = [qas[i]['question'] for i in range(len(qas))]\n",
    "        answer_li = [qas[i]['answers'][0]['text'] for i in range(len(qas))]\n",
    "        context_li = [dev_squad_data['data'][idx1]['paragraphs'][idx2]['context']] * len(qas)\n",
    "\n",
    "        for context, question, answer in zip(context_li, question_li, answer_li):\n",
    "            dev_qa_data.append({\n",
    "                \"context\": context, \n",
    "                \"question\": question, \n",
    "                \"answer\": answer\n",
    "            })\n",
    "test_dataset = Dataset.from_pandas(pd.DataFrame(dev_qa_data))\n",
    "test_dataset = test_dataset.map(lambda x: tokenize_gen_answer(x, tokenizer), batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "67cf9e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\n",
      "To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\n",
      "Saint Bernadette Soubirous\n",
      "[2788, 8942, 9, 26, 1954, 264, 8371, 8283, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "Saint Bernadette Soubirous</s>\n"
     ]
    }
   ],
   "source": [
    "train_sample = train_dataset[0]\n",
    "print(train_sample[\"context\"])\n",
    "print(train_sample[\"question\"])\n",
    "print(train_sample[\"answer\"])\n",
    "print(train_sample[\"labels\"])\n",
    "print(tokenizer.decode([id for id in train_sample[\"labels\"] if id != -100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0ecc3a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 884,736 || all params: 248,462,592 || trainable%: 0.3561\n"
     ]
    }
   ],
   "source": [
    "# LoRA 적용되었는지 확인\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a050aea4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.13497143983840942,\n",
       " 'eval_runtime': 1.2666,\n",
       " 'eval_samples_per_second': 78.951,\n",
       " 'eval_steps_per_second': 10.264,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 평가에서 loss 확인\n",
    "trainer.evaluate(tokenized_dataset.select(range(100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c5addd",
   "metadata": {},
   "source": [
    "- Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7dc5d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=GENERATE_ANSWER_MODEL,      # 모델 저장 경로\n",
    "    per_device_train_batch_size=4,         # 배치 크기\n",
    "    num_train_epochs=3,                    # 학습 epoch 수\n",
    "    learning_rate=5e-5,                    # 학습률 (조금 낮게 설정 권장)\n",
    "    logging_strategy=\"steps\",              # 로깅 전략\n",
    "    logging_steps=200,   \n",
    "    logging_first_step=True,               #  50 step마다 로그 출력\n",
    "    save_strategy=\"epoch\",                 # epoch 단위로 저장\n",
    "    save_total_limit=2,                    # 최대 2개 모델만 저장\n",
    "    # evaluation_strategy=\"steps\",           # 일정 step마다 평가\n",
    "    eval_steps=500,                        # 평가 주기\n",
    "    # predict_with_generate=True,            # generate() 기반 평가 활성화\n",
    "    fp16=True,                             # GPU가 FP16 지원 시 속도 ↑\n",
    "    report_to=\"none\"                       # wandb 등 외부 리포팅 비활성화\n",
    ")\n",
    "\n",
    "# Data collator (padding, label shift 자동 처리)\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "# Trainer 생성\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=train_dataset, \n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "# Save final model\n",
    "trainer.save_model(GENERATE_ANSWER_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d21c2a1",
   "metadata": {},
   "source": [
    "- Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31d0e731",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gayeon39/miniconda3/envs/da/lib/python3.12/site-packages/accelerate/utils/modeling.py:1614: UserWarning: The following device_map keys do not match any submodules in the model: ['decoder.embed_tokens', 'encoder.embed_tokens']\n",
      "  warnings.warn(\n",
      "We've detected an older driver with an RTX 4000 series GPU. These drivers have issues with P2P. This can affect the multi-gpu inference when using accelerate device_map.Please make sure to update your driver to the latest version which resolves this.\n",
      "/tmp/ipykernel_2485545/626082772.py:8: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# Load the fine-tuned model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(GENERATE_ANSWER_MODEL)\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(BASE_MODEL, device_map=\"auto\", torch_dtype=torch.float16)\n",
    "# Load the PEFT model\n",
    "model = PeftModel.from_pretrained(base_model, GENERATE_ANSWER_MODEL)\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "# results = trainer.evaluate(test_dataset)\n",
    "# print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc07a9fb",
   "metadata": {},
   "source": [
    "- Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46422625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: [PAR] [TLE] The Oberoi Group [SEP] The Oberoi Group is a hotel company with its head office in Delhi.  Founded in 1934, the company owns and/or operates 30+ luxury hotels and two river cruise ships in six countries, primarily under its Oberoi Hotels & Resorts and Trident Hotels brands. [PAR] [TLE] Oberoi family [SEP] The Oberoi family is an Indian family that is famous for its involvement in hotels, namely through The Oberoi Group. \n",
      "Question: The Oberoi family is part of a hotel company that has a head office in what city?\n",
      "Generated Answer: Delhi. Founded in 1934\n",
      "Answer: Delhi\n"
     ]
    }
   ],
   "source": [
    "# Define a function to generate answers based on context and question\n",
    "def generate_answer(context, question):\n",
    "    \"\"\"\n",
    "    Context와 Question이 주어졌을 때 Answer를 생성하는 함수\n",
    "    \"\"\"\n",
    "    # T5 모델용 프롬프트 생성 (question answering format)\n",
    "    prompt = f\"question: {question} context: {context}\"\n",
    "    \n",
    "    # Tokenize the prompt\n",
    "    inputs = tokenizer(\n",
    "        prompt, \n",
    "        return_tensors=\"pt\", \n",
    "        padding=\"max_length\", \n",
    "        truncation=True, \n",
    "        max_length=512\n",
    "    )\n",
    "    \n",
    "    # Generate the answer using the model\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"], \n",
    "        attention_mask=inputs[\"attention_mask\"], \n",
    "        max_length=128,     # Answer는 보통 question보다 길 수 있음\n",
    "        min_length=5,       # 최소 길이 설정\n",
    "        num_beams=4,        # Beam search for better quality\n",
    "        early_stopping=True,\n",
    "        do_sample=False,    # Deterministic generation\n",
    "        temperature=1.0     # 필요시 조정\n",
    "    )\n",
    "    \n",
    "    # Decode the generated answer\n",
    "    generated_answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated_answer\n",
    "\n",
    "# Generate the answer\n",
    "generated_answer = generate_answer(target_context[0], target_question[0])\n",
    "\n",
    "# Print the generated answer\n",
    "print(f\"Context: {target_context[0]}\")\n",
    "print(f\"Question: {target_question[0]}\")\n",
    "print(f\"Generated Answer: {generated_answer}\")\n",
    "print(f\"Answer: {TRAIN_HOTPOT[0]['qas'][0]['answers'][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753a8636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Answer Extraction용 데이터셋\n",
    "# qa_data = [\n",
    "#     {\n",
    "#         \"context\": \"NASA is the United States government agency responsible for the civilian space program, as well as aeronautics and space research.\",\n",
    "#         \"question\": \"What does NASA stand for?\",\n",
    "#         \"answer\": \"National Aeronautics and Space Administration\"\n",
    "#     },\n",
    "#     {\n",
    "#         \"context\": \"Photosynthesis is the process by which green plants use sunlight to synthesize foods from carbon dioxide and water. This process occurs mainly in the leaves.\",\n",
    "#         \"question\": \"What is photosynthesis?\", \n",
    "#         \"answer\": \"the process by which green plants use sunlight to synthesize foods from carbon dioxide and water\"\n",
    "#     },\n",
    "#     {\n",
    "#         \"context\": \"Albert Einstein was a German-born theoretical physicist who developed the theory of relativity, one of the two pillars of modern physics.\",\n",
    "#         \"question\": \"Who was Albert Einstein?\",\n",
    "#         \"answer\": \"a German-born theoretical physicist who developed the theory of relativity\"\n",
    "#     },\n",
    "# ]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "da",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
